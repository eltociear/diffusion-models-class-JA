# Introduction √† ü§ó Diffusers

<CourseFloatingBanner unit={1}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Introduction to Diffusers", value: "https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/units/fr/unit1/introduction_to_diffusers.ipynb"},
    {label: "Introduction to Diffusers", value: "https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/units/fr/unit1/introduction_to_diffusers.ipynb"},
]} />


Dans ce *notebook*, vous allez entra√Æner votre premier mod√®le de diffusion pour g√©n√©rer des images de mignons papillons ü¶ã. 
En cours de route, vous apprendrez les composants de base de la biblioth√®que ü§ó *Diffusers*, qui fournira une bonne assise pour les applications plus avanc√©es que nous couvrirons plus tard dans le cours.

D√©butons par une vue d'ensemble de ce qu'on va faire dans ce *notebook*. Nous allons :
- Voir un puissant pipeline de mod√®les de diffusion personnalis√© en action (avec des informations sur la fa√ßon de cr√©er votre propre version).
- Cr√©er votre propre mini-pipeline en :
	- R√©capitulant les id√©es principales derri√®re les mod√®les de diffusion
    - Chargement de donn√©es √† partir du Hub pour l'entra√Ænement
    - Explorer comment ajouter du bruit √† ces donn√©es √† l'aide d'un planificateur
    - Cr√©er et entra√Æner le mod√®le UNet
    - Rassembler les pi√®ces du puzzle pour en faire un pipeline fonctionnel
- √âditer et ex√©cuter un script pour initialiser des s√©ries d'entra√Ænement plus longues, qui g√®rera
	- Entra√Ænement multi-GPU via ü§ó *Accelerate*
	- Journalisation de l'exp√©rience pour suivre les statistiques critiques
	- T√©l√©chargement du mod√®le final sur le *Hub* d'*Hugging Face*


## Installation des biblioth√®ques

Ex√©cutez la cellule suivante pour installer la biblioth√®que ü§ó *Diffusers* ainsi que quelques autres pr√©requis :

```py
%pip install -qq -U diffusers datasets transformers accelerate ftfy pyarrow==9.0.0
```

Ensuite, rendez-vous sur https://huggingface.co/settings/tokens et cr√©ez un *tokens* d'acc√®s avec autorisation d'√©criture si vous n'en avez pas d√©j√† un :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Vous pouvez vous connecter avec ce token en utilisant la ligne de commande (`huggingface-cli login`) ou en ex√©cutant la cellule suivante :

```py
from huggingface_hub import notebook_login

notebook_login()
```

Vous devez ensuite installer Git-LFS pour t√©l√©charger les *checkpoints* de votre mod√®le :

```py
%%capture
!sudo apt -qq install git-lfs
!git config --global credential.helper store
```

Enfin, importons les biblioth√®ques que nous utiliserons et d√©finissons quelques fonctions de confort que nous utiliserons plus tard dans le *notebook* :

```py
import numpy as np
import torch
import torch.nn.functional as F
from matplotlib import pyplot as plt
from PIL import Image


def show_images(x):
    """√âtant donn√© un lot d'images x, faire une grille et convertir en PIL"""
    x = x * 0.5 + 0.5  # On va de (-1, 1) et revenons (0, 1)
    grid = torchvision.utils.make_grid(x)
    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255
    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))
    return grid_im


def make_grid(images, size=64):
    """√âtant donn√© une liste d'images PIL, les empiler en une ligne pour faciliter la visualisation."""
    output_im = Image.new("RGB", (size * len(images), size))
    for i, im in enumerate(images):
        output_im.paste(im.resize((size, size)), (i * size, 0))
    return output_im


# Les utilisateurs de Mac peuvent avoir besoin de device = 'mps' (non test√©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```


OK, nous sommes pr√™ts !

## Exemple g√©n√©rique d'inf√©rence avec Dreambooth, un avant-go√ªt de ce qui nous attend

Si vous avez un tant soit peu consult√© les m√©dias sociaux au cours des derniers mois, vous avez certainement entendu parler de *Stable Diffusion*. Il s'agit d'un puissant mod√®le de diffusion latent conditionn√© par le texte (ne vous inqui√©tez pas, nous allons apprendre ce que cela signifie). Mais il a un d√©faut : il ne sait pas √† quoi vous ou moi ressemblons, √† moins que nous soyons suffisamment c√©l√®bres pour que nos images soient r√©pandues sur internet.

Dreambooth nous permet de cr√©er notre propre variante de mod√®le avec une connaissance suppl√©mentaire d'un visage, d'un objet ou d'un style sp√©cifique. Le Corridor Crew a r√©alis√© une excellente vid√©o (en anglais) en utilisant cette technique pour raconter des histoires avec des personnages coh√©rents, ce qui est un excellent exemple de ce que cette technique peut faire :

```py
from IPython.display import YouTubeVideo

YouTubeVideo("W4Mcuh38wyM")
```

Voici un exemple d'une sortie d'un [mod√®le](https://huggingface.co/sd-dreambooth-library/mr-potato-head) entra√Æn√© sur 5 photos du jouet Monsieur Patate.

Tout d'abord, nous chargeons le pipeline. Ceci t√©l√©charge les poids du mod√®le depuis le Hub. √âtant donn√© que plusieurs gigaoctets de donn√©es sont t√©l√©charg√©s pour une d√©monstration d'une ligne, vous pouvez sauter cette cellule et simplement admirer la sortie de l'exemple !

```py
from diffusers import StableDiffusionPipeline

# Consultez https://huggingface.co/sd-dreambooth-library pour d√©couvrir de nombreux mod√®les provenant de la communaut√©
model_id = "sd-dreambooth-library/mr-potato-head"

# Chargement du pipeline
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(
    device
)
```

Une fois le chargement du pipeline termin√©, nous pouvons g√©n√©rer des images avec :

```py
prompt = "an abstract oil painting of sks mr potato head by picasso"
image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]
image
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


<Tip>
‚úèÔ∏è *A votre tour !* essayez vous-m√™me avec des prompts diff√©rents. Le *token* `sks` repr√©sente un identifiant unique pour le nouveau concept, que se passe-t-il si vous l'omettez ? Vous pouvez aussi exp√©rimenter en changeant le nombre de pas d'√©chantillonnage (jusqu'o√π pouvez-vous descendre ?) et le param√®tre `guidance_scale`, qui d√©termine jusqu'√† quel point le mod√®le va essayer de correspondre au prompt.

Il se passe beaucoup de choses dans ce pipeline ! √Ä la fin du cours, vous saurez comment tout cela fonctionne. Pour l'instant, voyons comment nous pouvons entra√Æner un mod√®le de diffusion √† partir de z√©ro.
</Tip>

## MVP (Minimum Viable Pipeline)

### Exemple d'inf√©rence sur les papillons
L'API de base de ü§ó *Diffusers* est divis√©e en trois composants principaux :
- **Pipelines** : classes de haut niveau con√ßues pour g√©n√©rer rapidement des √©chantillons √† partir de mod√®les de diffusion populaires entra√Æn√©s de mani√®re conviviale.
- **Models** : architectures populaires pour entra√Æner de nouveaux mod√®les de diffusion, par exemple [UNet](https://arxiv.org/abs/1505.04597).
- **Schedulers** : diverses techniques pour g√©n√©rer des images √† partir du bruit pendant l'*inf√©rence* ainsi que pour g√©n√©rer des images bruit√©es pour l'*entra√Ænement*.

Les pipelines sont parfaits pour les utilisateurs finaux, mais si vous √™tes ici pour ce cours, nous supposons que vous voulez savoir ce qui se passe sous le capot ! Dans le reste de ce *notebook*, nous allons donc construire notre propre pipeline capable de g√©n√©rer de petites images de papillons. Voici le r√©sultat final en action :

```py
from diffusers import DDPMPipeline

# Chargement du pipeline de papillons
butterfly_pipeline = DDPMPipeline.from_pretrained(
    "johnowhitaker/ddpm-butterflies-32px"
).to(device)

# Cr√©ation de 8 images
images = butterfly_pipeline(batch_size=8).images

# Visualisation du r√©sultat
make_grid(images)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


Ce n'est peut-√™tre pas aussi impressionnant que l'exemple de DreamBooth, mais nous entra√Ænons notre mod√®le √† partir de z√©ro avec ~0,0001% des donn√©es utilis√©es pour entra√Æner Stable Diffusion. En parlant d'entra√Ænement, rappelez-vous que l'entra√Ænement d'un mod√®le de diffusion ressemble √† ceci :
- Chargement de quelques images √† partir des donn√©es entra√Æn√©es.
- Ajout de bruit, en diff√©rentes quantit√©s.
- Introduction des versions bruit√©es des donn√©es d'entr√©e dans le mod√®le.
- √âvaluation de la capacit√© du mod√®le √† d√©bruiter ces donn√©es d'entr√©e
- Utilisation de ces informations pour mettre √† jour les poids du mod√®le, et r√©p√©tition.

Nous allons explorer ces √©tapes une par une dans les prochaines parties jusqu'√† ce que nous ayons une boucle d'entra√Ænement compl√®te, puis nous verrons comment √©chantillonner √† partir du mod√®le entra√Æn√© et comment regrouper le tout dans un pipeline pour faciliter le partage. Commen√ßons par les donn√©es.

### T√©l√©charger le jeu de donn√©es d'entra√Ænement

Pour cet exemple, nous utilisons un jeu de donn√©es d'images provenant du *Hub* d'*Hugging Face*. Plus pr√©cis√©ment, cette collection de [1000 images de papillons](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset). Il s'agit d'un tr√®s petit jeu de donn√©es, c'est pourquoi nous avons aussi inclus des lignes en commentaires pour quelques options plus importantes. Si vous pr√©f√©rez utiliser votre propre collection d'images, vous pouvez √©galement utiliser l'exemple de code comment√© pour charger des images √† partir d'un dossier.

```py
import torchvision
from datasets import load_dataset
from torchvision import transforms

dataset = load_dataset("huggan/smithsonian_butterflies_subset", split="train")

# Ou charger des images √† partir d'un dossier local
# dataset = load_dataset("imagefolder", data_dir="path/to/folder")

# Nous entra√Ænerons sur des images carr√©es de 32 pixels, mais vous pouvez aussi essayer des tailles plus grandes
image_size = 32
# Vous pouvez r√©duire la taille de votre batch si vous manquez de m√©moire GPU
batch_size = 64

# D√©finition les augmentations de donn√©es
preprocess = transforms.Compose(
    [
        transforms.Resize((image_size, image_size)),  # Redimensionner
        transforms.RandomHorizontalFlip(),  # Retournement al√©atoire
        transforms.ToTensor(),  # Convertir en tenseur (0, 1)
        transforms.Normalize([0.5], [0.5]),  # Passage en (-1, 1)
    ]
)


def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}


dataset.set_transform(transform)

# Cr√©er un chargeur de donn√©es √† partir du jeu de donn√©es pour servir les images transform√©es en batchs
train_dataloader = torch.utils.data.DataLoader(
    dataset, batch_size=batch_size, shuffle=True
)
```

Nous pouvons saisir un batch d'images et en visualiser quelques-unes comme suit :

```py
xb = next(iter(train_dataloader))["images"].to(device)[:8]
print("X shape:", xb.shape)
show_images(xb).resize((8 * 64, 64), resample=Image.NEAREST)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


Nous nous en tenons √† un petit jeu de donn√©es avec des images de 32 pixels pour que les temps d'entra√Ænement restent raisonnables dans ce *notebook*.

## D√©finir le planificateur

Notre plan d'entra√Ænement consiste √† prendre ces images d'entr√©e et √† leur ajouter du bruit, puis √† transmettre les images bruit√©es au mod√®le. Lors de l'inf√©rence, nous utiliserons les pr√©dictions du mod√®le pour supprimer le bruit de mani√®re it√©rative. Dans ü§ó *Diffusers*, ces deux processus sont g√©r√©s par le *scheduler* (planificateur).

Le planificateur de bruit d√©termine la quantit√© de bruit ajout√©e √† diff√©rents moments. Voici comment nous pourrions cr√©er un planificateur en utilisant les param√®tres par d√©faut pour l'entra√Ænement et l'√©chantillonnage "DDPM" (d'apr√®s l'article d'apr√®s l'article [*Denoising Diffusion Probabalistic Models*](https://arxiv.org/abs/2006.11239)) :

```py
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
```

Le papier DDPM d√©crit un processus de corruption qui ajoute une petite quantit√© de bruit √† chaque pas de temps. √âtant donn√© $x_{t-1}$ pour un certain pas de temps, nous pouvons obtenir la version suivante (l√©g√®rement plus bruyante) $x_t$ avec :  

$q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}) \quad
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$<br><br>


Nous prenons $x_{t-1}$, l'√©chelonnons de $\sqrt{1 - \beta_t}$ et ajoutons du bruit √©chelonn√© par $\beta_t$. Ce $\beta$ est d√©fini pour chaque $t$ selon un certain planificateur et d√©termine la quantit√© de bruit ajout√©e par pas de temps. Maintenant, nous ne voulons pas n√©cessairement faire cette op√©ration 500 fois pour obtenir $x_{500}$, nous avons donc une autre formule pour obtenir $x_t$ pour n'importe quel t √©tant donn√© $x_0$ :

$\begin{aligned}
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, {(1 - \bar{\alpha}_t)} \mathbf{I})
\end{aligned}$ where $\bar{\alpha}_t = \prod_{i=1}^T \alpha_i$ and $\alpha_i = 1-\beta_i$<br><br>

La notation math√©matique fait toujours peur ! Heureusement, le planificateur s'en charge pour nous. Nous pouvons tracer $\sqrt{\bar{\alpha}_t}$ (appel√© `sqrt_alpha_prod`) et $\sqrt{(1 - \bar{\alpha}_t)}$ (appel√© `sqrt_one_minus_alpha_prod`) pour voir comment l'entr√©e ($x$) et le bruit sont mis √† l'√©chelle et m√©lang√©s √† travers diff√©rents pas de temps :

```py
plt.plot(noise_scheduler.alphas_cumprod.cpu() ** 0.5, label=r"${\sqrt{\bar{\alpha}_t}}$")
plt.plot((1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5, label=r"$\sqrt{(1 - \bar{\alpha}_t)}$")
plt.legend(fontsize="x-large");
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


<Tip>

‚úèÔ∏è *A votre tour !* Vous pouvez explorer comment ce graphique change avec diff√©rents param√®tres pour `beta_start`, `beta_end` et `beta_schedule` en rempla√ßant l'une des options comment√©es ci-dessous :

</Tip>

```py
## Exemple avec beaucoup de bruit ajout√© :
# noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.004)

## Le planificateur cosinus pouvant s'av√©rer meilleur pour les images de petite taille :
# noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')
```

Quel que soit le planificateur que vous avez choisi, nous pouvons maintenant l'utiliser pour ajouter du bruit en diff√©rentes quantit√©s en utilisant la fonction `noise_scheduler.add_noise` comme suit :

```py
timesteps = torch.linspace(0, 999, 8).long().to(device)
noise = torch.randn_like(xb)
noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)
print("Noisy X shape", noisy_xb.shape)
show_images(noisy_xb).resize((8 * 64, 64), resample=Image.NEAREST)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

L√† encore, √©tudiez l'effet de l'utilisation de diff√©rents planificateurs et param√®tres de bruit. Cette [vid√©o](https://www.youtube.com/watch?v=fbLgFrlTnGU) (en anglais) explique en d√©tail certains des calculs ci-dessus et constitue une excellente introduction √† certains de ces concepts.


### D√©finir le mod√®le

Nous en arrivons maintenant √† l'√©l√©ment central : le mod√®le lui-m√™me.

La plupart des mod√®les de diffusion utilisent des architectures qui sont des variantes d'un [U-net](https://arxiv.org/abs/1505.04597) et c'est ce que nous utiliserons ici.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

En bref :
- l'image en entr√©e du mod√®le passe par plusieurs blocs de couches ResNet, chacun divisant la taille de l'image par 2
- puis elle passe √† travers le m√™me nombre de blocs qui la sur√©chantillonnent.
- il y a des *skip connections* qui relient les caract√©ristiques sur le chemin du sous-√©chantillonnage aux couches correspondantes dans le chemin du sur√©chantillonnage.

L'une des principales caract√©ristiques de ce mod√®le est qu'il pr√©dit des images de la m√™me taille que l'entr√©e, ce qui est exactement ce dont nous avons besoin ici.

ü§ó *Diffusers* nous fournit une classe `UNet2DModel` pratique qui cr√©e l'architecture d√©sir√©e dans PyTorch.

Cr√©ons un U-net pour la taille d'image d√©sir√©e. Notez que les `down_block_types` correspondent aux blocs de sous-√©chantillonnage (en vert sur le diagramme ci-dessus), et que les `up_block_types` sont les blocs de sur√©chantillonnage (en rouge sur le diagramme) :


```py
from diffusers import UNet2DModel

# Cr√©ation d'un mod√®le
model = UNet2DModel(
    sample_size=image_size,  # la r√©solution de l'image cible
    in_channels=3,  # le nombre de canaux d'entr√©e, 3 pour les images RVB
    out_channels=3,  # le nombre de canaux de sortie
    layers_per_block=2,  # le nombre de couches ResNet √† utiliser par bloc UNet
    block_out_channels=(64, 128, 128, 256),  # Plus de canaux -> plus de param√®tres
    down_block_types=(
        "DownBlock2D",  # un bloc de sous-√©chantillonnage ResNet standard
        "DownBlock2D",
        "AttnDownBlock2D",  # un bloc de sous-√©chantillonnage ResNet avec auto-attention spatiale
        "AttnDownBlock2D",
    ),
    up_block_types=(
        "AttnUpBlock2D",
        "AttnUpBlock2D",  # un bloc de sur√©chantillonnage ResNet avec auto-attention spatiale
        "UpBlock2D",
        "UpBlock2D",  # un bloc de sur√©chantillonnage ResNet standard
    ),
)
model.to(device)
```


Lorsque vous traitez des donn√©es d'entr√©e en haute r√©solution, vous pouvez utiliser davantage de blocs descendants et ascendants, et ne conserver les couches d'attention que pour les couches de r√©solution les plus basses (inf√©rieures) afin de r√©duire l'utilisation de la m√©moire. Nous verrons plus tard comment vous pouvez exp√©rimenter pour trouver les meilleurs param√®tres pour votre cas d'utilisation.

Nous pouvons v√©rifier que le passage d'un batch de donn√©es et de pas de temps al√©atoires produit une sortie de m√™me forme que les donn√©es d'entr√©e :

```py
with torch.no_grad():
    model_prediction = model(noisy_xb, timesteps).sample
model_prediction.shape
```

Dans la section suivante, nous verrons comment entra√Æner ce mod√®le.

### Cr√©er une boucle d'entra√Ænement

Il est temps d'entra√Æner ! Voici une boucle d'optimisation typique dans PyTorch, o√π nous parcourons les donn√©es batch par batch et mettons √† jour les param√®tres de notre mod√®le √† chaque √©tape √† l'aide d'un optimiseur, ici, l'optimiseur AdamW avec un taux d'apprentissage de 0,0004.

Pour chaque batch de donn√©es, nous
- √©chantillonnons des pas de temps al√©atoires
- bruitons les donn√©es en cons√©quence
- transmettons les donn√©es bruit√©es au mod√®le
- comparons les pr√©dictions du mod√®le avec la cible (c'est-√†-dire le bruit dans ce cas) en utilisant l'erreur quadratique moyenne comme fonction de perte
- mettons √† jour les param√®tres du mod√®le via `loss.backward()` et `optimizer.step()`.

Au cours de ce processus, nous enregistrons aussi les pertes au fil du temps pour un trac√© ult√©rieur.

NB : ce code prend pr√®s de 10 minutes √† ex√©cuter. N'h√©sitez pas √† sauter ces deux cellules et √† utiliser le mod√®le pr√©-entra√Æn√© si vous √™tes press√©. Vous pouvez √©galement √©tudier comment la r√©duction du nombre de canaux dans chaque couche via la d√©finition du mod√®le ci-dessus peut acc√©l√©rer les choses.

L'[exemple officiel d'entra√Ænement](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) de ü§ó *Diffusers* entra√Æne un mod√®le plus grand sur ce jeu de donn√©es √† une r√©solution plus √©lev√©e, et constitue une bonne r√©f√©rence pour ce √† quoi ressemble une boucle d'entra√Ænement moins minimale :

```py
# D√©finir le planificateur de bruit
noise_scheduler = DDPMScheduler(
    num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2"
)

# Boucle d'entra√Ænement
optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)

losses = []

for epoch in range(30):
    for step, batch in enumerate(train_dataloader):
        clean_images = batch["images"].to(device)
        # Exemple de bruit √† ajouter aux images
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]

        # √âchantillonner un pas de temps al√©atoire pour chaque image
        timesteps = torch.randint(
            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device
        ).long()

        # Ajouter du bruit aux images propres en fonction de l'ampleur du bruit √† chaque √©tape
        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

        # Obtenir la pr√©diction du mod√®le
        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]

        # Calculer la perte
        loss = F.mse_loss(noise_pred, noise)
        loss.backward(loss)
        losses.append(loss.item())

        # Mise √† jour des param√®tres du mod√®le √† l'aide de l'optimiseur
        optimizer.step()
        optimizer.zero_grad()

    if (epoch + 1) % 5 == 0:
        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)
        print(f"Epoch:{epoch+1}, loss: {loss_last_epoch}")
```

En tra√ßant la perte, nous constatons que le mod√®le s'am√©liore rapidement dans un premier temps, puis continue √† s'am√©liorer √† un rythme plus lent (ce qui est plus √©vident si nous utilisons une √©chelle logarithmique, comme indiqu√© √† droite) :

```py
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
axs[0].plot(losses)
axs[1].plot(np.log(losses))
plt.show()
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Au lieu d'ex√©cuter le code d'entra√Ænement ci-dessus, vous pouvez utiliser le mod√®le du pipeline comme suit :

```py
## D√©commenter pour charger le mod√®le que j'ai entra√Æn√© plus t√¥t √† la place :
# model = butterfly_pipeline.unet
```

### G√©n√©rer des images

Comment obtenir des images avec ce mod√®le ?  

‚Ä¢ Option 1 : Cr√©ation d'un pipeline :

```py
from diffusers import DDPMPipeline

image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)

pipeline_output = image_pipe()
pipeline_output.images[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous pouvons enregistrer un pipeline dans un dossier local comme suit :

```py
image_pipe.save_pretrained("my_pipeline")
```

Inspection du contenu du dossier :

```py
!ls my_pipeline/
```

```py
model_index.json  scheduler  unet
```

Les sous-dossiers `scheduler` et `unet` contiennent tout ce qui est n√©cessaire pour recr√©er ces composants. Par exemple, dans le dossier `unet` vous trouverez les poids du mod√®le (`diffusion_pytorch_model.bin`) ainsi qu'un fichier de configuration qui sp√©cifie l'architecture UNet.

```py
!ls my_pipeline/unet/
```

```py
config.json  diffusion_pytorch_model.bin
```

Ensemble, ces fichiers contiennent tout ce qui est n√©cessaire pour recr√©er le pipeline. Vous pouvez les t√©l√©charger manuellement sur le *Hub* pour partager le pipeline avec d'autres personnes, ou consulter le code pour le faire via l'API dans la section suivante.

‚Ä¢ Option 2 : √©crire une boucle d'√©chantillonnage

Si vous inspectez la m√©thode `forward` du pipeline, vous pourrez voir ce qui se passe lorsque nous lan√ßons `image_pipe()` :

```py
# ??image_pipe.forward
```

Nous commen√ßons par un bruit al√©atoire et parcourons les pas de temps de l'ordonnanceur du plus bruyant au moins bruyant, en supprimant une petite quantit√© de bruit √† chaque √©tape sur la base de la pr√©diction du mod√®le :

```py
# Point de d√©part al√©atoire (8 images al√©atoires) :
sample = torch.randn(8, 3, 32, 32).to(device)

for i, t in enumerate(noise_scheduler.timesteps):

    # Obtenir le mod√®le de pr√©diction
    with torch.no_grad():
        residual = model(sample, t).sample

    # Mise √† jour de l'√©chantillon avec le pas
    sample = noise_scheduler.step(residual, t, sample).prev_sample

show_images(sample)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

La fonction `noise_scheduler.step()` effectue les calculs n√©cessaires pour mettre √† jour `sample` de mani√®re appropri√©e. Il existe un certain nombre de m√©thodes d'√©chantillonnage. Dans l'unit√© suivante, nous verrons comment nous pouvons √©changer un √©chantillonneur diff√©rent pour acc√©l√©rer la g√©n√©ration d'images avec des mod√®les existants, et nous parlerons plus en d√©tail de la th√©orie derri√®re l'√©chantillonnage des mod√®les de diffusion.

### Pousser votre mod√®le vers le *Hub*

Dans l'exemple ci-dessus, nous avons enregistr√© notre pipeline dans un dossier local. Pour pousser notre mod√®le vers le *Hub*, nous aurons besoin d'un d√©p√¥t de mod√®les dans lequel nous pourrons pousser nos fichiers. Nous d√©terminerons le nom du d√©p√¥t √† partir de l'ID du mod√®le que nous voulons donner √† notre mod√®le (n'h√©sitez pas √† remplacer le nom du mod√®le par votre propre choix ; il doit juste contenir votre nom d'utilisateur, ce que fait la fonction `get_full_repo_name()`) :

```py
from huggingface_hub import get_full_repo_name

model_name = "sd-class-butterflies-32"
hub_model_id = get_full_repo_name(model_name)
hub_model_id
```

Ensuite, cr√©er un d√©p√¥t de mod√®le sur le ü§ó *Hub* et pousser notre mod√®le :

```py
from huggingface_hub import HfApi, create_repo

create_repo(hub_model_id)
api = HfApi()
api.upload_folder(
    folder_path="my_pipeline/scheduler", path_in_repo="", repo_id=hub_model_id
)
api.upload_folder(folder_path="my_pipeline/unet", path_in_repo="", repo_id=hub_model_id)
api.upload_file(
    path_or_fileobj="my_pipeline/model_index.json",
    path_in_repo="model_index.json",
    repo_id=hub_model_id,
)
```

La derni√®re chose √† faire est de cr√©er une belle carte mod√®le afin que notre g√©n√©rateur de papillons puisse √™tre facilement trouv√© sur le ü§ó *Hub* (n'h√©sitez pas √† d√©velopper et √† modifier la description !) :

```py
from huggingface_hub import ModelCard

content = f"""
---
license: mit
tags:
- pytorch
- diffusers
- unconditional-image-generation
- diffusion-models-class
---

# Model Card for Unit 1 of the [Diffusion Models Class üß®](https://github.com/huggingface/diffusion-models-class)

This model is a diffusion model for unconditional image generation of cute ü¶ã.

## Usage


```python
from diffusers import DDPMPipeline

pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')
image = pipeline().images[0]
image
```
"""

card = ModelCard(content)
card.push_to_hub(hub_model_id)
```

Maintenant que le mod√®le est sur le *Hub*, vous pouvez le t√©l√©charger de n'importe o√π en utilisant la m√©thode `from_pretrained()` de `DDPMPipeline` comme suit :

```py
from diffusers import DDPMPipeline

image_pipe = DDPMPipeline.from_pretrained(hub_model_id)
pipeline_output = image_pipe()
pipeline_output.images[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Bien, √ßa marche !

## Passer √† l'√©chelle sup√©rieure avec ü§ó *Accelerate*
Ce *notebook* a √©t√© con√ßu √† des fins d'apprentissage, et en tant que tel, nous avons essay√© de garder le code aussi minimal et propre que possible. Pour cette raison, nous avons omis certaines choses que vous pourriez souhaiter si vous deviez entra√Æner un mod√®le plus grand sur beaucoup plus de donn√©es, comme le support multi-GPU, la trace de la progression et des images d'exemple, la sauvegarde du gradient pour supporter des tailles de batch plus importantes, le t√©l√©chargement automatique des mod√®les et ainsi de suite. Heureusement, la plupart de ces fonctionnalit√©s sont disponibles dans l'exemple de script d'entra√Ænement [ici](https://github.com/huggingface/diffusers/raw/main/examples/unconditional_image_generation/train_unconditional.py)..

Vous pouvez t√©l√©charger le fichier comme suit :

```py
!wget https://github.com/huggingface/diffusers/raw/main/examples/unconditional_image_generation/train_unconditional.py
```

Ouvrez le fichier et vous verrez o√π le mod√®le est d√©fini et quels sont les param√®tres disponibles. Nous ex√©cutons le script √† l'aide de la commande suivante :

```py
# Donnons un nom √† notre nouveau mod√®le pour le Hub
model_name = "sd-class-butterflies-64"
hub_model_id = get_full_repo_name(model_name)
```

```py
!accelerate launch train_unconditional.py \
  --dataset_name="huggan/smithsonian_butterflies_subset" \
  --resolution=64 \
  --output_dir={model_name} \
  --train_batch_size=32 \
  --num_epochs=50 \
  --gradient_accumulation_steps=1 \
  --learning_rate=1e-4 \
  --lr_warmup_steps=500 \
  --mixed_precision="no"
```

Comme pr√©c√©demment, poussons le mod√®le vers le *Hub* et cr√©ons une belle carte de mod√®le (et n'h√©sitez pas √† l'√©diter comme vous le souhaitez !):

```py
create_repo(hub_model_id)
api = HfApi()
api.upload_folder(
    folder_path=f"{model_name}/scheduler", path_in_repo="", repo_id=hub_model_id
)
api.upload_folder(
    folder_path=f"{model_name}/unet", path_in_repo="", repo_id=hub_model_id
)
api.upload_file(
    path_or_fileobj=f"{model_name}/model_index.json",
    path_in_repo="model_index.json",
    repo_id=hub_model_id,
)

content = f"""
---
license: mit
tags:
- pytorch
- diffusers
- unconditional-image-generation
- diffusion-models-class
---

# Model Card for Unit 1 of the [Diffusion Models Class üß®](https://github.com/huggingface/diffusion-models-class)

This model is a diffusion model for unconditional image generation of cute ü¶ã.

## Usage

```python
from diffusers import DDPMPipeline

pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')
image = pipeline().images[0]
image
```
"""

card = ModelCard(content)
card.push_to_hub(hub_model_id)
```

Environ 45 minutes plus tard, voici le r√©sultat :

```py
pipeline = DDPMPipeline.from_pretrained(hub_model_id).to(device)
images = pipeline(batch_size=8).images
make_grid(images)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

<Tip>

‚úèÔ∏è *A votre tour !* essayez de trouver des param√®tres d'entra√Ænement/de mod√®le qui donnent de bons r√©sultats en un minimum de temps, et partagez vos r√©sultats avec la communaut√©. Fouillez dans le script pour voir si vous pouvez comprendre le code, et demandez des √©claircissements sur tout ce qui vous semble confus.

</Tip>

# Pistes d'approfondissement

Nous esp√©rons vous avoir donn√© un avant-go√ªt de ce que vous pouvez faire avec la biblioth√®que ü§ó *Diffusers* ! Voici quelques pistes possibles pour la suite :
- Essayez d'entra√Æner un mod√®le de diffusion inconditionnel sur un nouveau jeu de donn√©es. Points bonus si vous en [cr√©ez un vous-m√™me](https://huggingface.co/docs/datasets/image_dataset). Vous pouvez trouver d'excellents jeux de donn√©es d'images pour cette t√¢che dans l'[organisation HugGan](https://huggingface.co/huggan) sur le *Hub*. Assurez-vous simplement de les sous-√©chantillonner si vous ne voulez pas attendre tr√®s longtemps pour que le mod√®le s'entra√Æne !
- Essayez DreamBooth pour cr√©er votre propre pipeline de Stable Diffusion personnalis√© en utilisant ce [Space]((https://huggingface.co/spaces/multimodalart/dreambooth-training) ou ce [*notebook*](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
- Modifiez le script d'entra√Ænement pour explorer diff√©rents hyperparam√®tres UNet (nombre de couches, canaux, etc.), diff√©rents sch√©mas de bruit, etc.
- Consultez le *notebook* [*Impl√©mentation √† partir de 0*](https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit1/diffusion_models_from_scratch.ipynb) pour une approche diff√©rente des id√©es fondamentales que nous avons abord√©es dans cette unit√©.