# D√©bruitage inverse des mod√®les de diffusion implicites (DDIM)

<CourseFloatingBanner unit={4}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
	{label: "D√©bruitage inverse des mod√®les de diffusion implicites (DDIM)", value: "https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/units/fr/unit4/ddim_inversion.ipynb"},
    {label: "D√©bruitage inverse des mod√®les de diffusion implicites (DDIM)", value: "https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/units/fr/unit4/ddim_inversion.ipynb"},

]} />

Dans ce *notebook*, nous allons explorer l'inversion, voir comment elle est li√©e √† l'√©chantillonnage, et l'appliquer √† la t√¢che d'√©dition d'images avec Stable Diffusion.
Ce que vous allez apprendre :
- Comment fonctionne l'√©chantillonnage DDIM
- √âchantillonneurs d√©terministes et stochastiques
- La th√©orie derri√®re l'inversion DDIM
- L'√©dition d'images avec l'inversion

Commen√ßons !

# Configuration
```py
# !pip install -q transformers diffusers accelerate
```     
```py
import torch
import requests
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
from io import BytesIO
from tqdm.auto import tqdm
from matplotlib import pyplot as plt
from torchvision import transforms as tfms
from diffusers import StableDiffusionPipeline, DDIMScheduler

# Une fonction utile pour plus tard
def load_image(url, size=None):
    response = requests.get(url,timeout=0.2)
    img = Image.open(BytesIO(response.content)).convert('RGB')
    if size is not None:
        img = img.resize(size)
    return img
```    
```py
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
   
## Chargement d'un pipeline existant

```py
# Charger un pipeline
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5").to(device)
```
```py
# Mettre en place un planificateur DDIM
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
```     
```py
# √âchantillon d'une image pour s'assurer que tout fonctionne bien
prompt = 'Beautiful DSLR Photograph of a penguin on the beach, golden hour'
negative_prompt = 'blurry, ugly, stock photo'
im = pipe(prompt, negative_prompt=negative_prompt).images[0]
im.resize((256, 256)) # redimensionner pour une meilleure visualisation
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


## Echantillonage DDIM

√Ä un moment donn√© $t$, l'image bruit√©e $x_t$ est un m√©lange de l'image originale ($x_0$) et de bruit ($\epsilon$). Voici la formule pour $x_t$ tir√©e de l'article DDIM, √† laquelle nous nous r√©f√©rerons dans cette section :

$$ x_t = \sqrt{\alpha_t}x_0 + \sqrt{1-\alpha_t}\epsilon $$

$\epsilon$ est un bruit gaussien de variance unitaire
$\alpha_t$ ("alpha") est la valeur qui est appel√©e de mani√®re confuse $\bar{\alpha}$ ("alpha_bar") dans le papier DDPM ( !!) et qui d√©finit le planificateur de bruit. Dans ü§ó *Diffusers*, le planificateur alpha est calcul√© et les valeurs sont stock√©es dans le `scheduler.alphas_cumprod`. Je sais que c'est d√©routant ! Tra√ßons ces valeurs, et n'oubliez pas que pour le reste de ce *notebook*, nous utiliserons la notation de DDIM.

```py
# Tracer 'alpha' (alpha_bar dans DDPM, alphas_cumprod dans Diffusers)
timesteps = pipe.scheduler.timesteps.cpu()
alphas = pipe.scheduler.alphas_cumprod[timesteps]
plt.plot(timesteps, alphas, label='alpha_t');
plt.legend()
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Au d√©part (√©tape 0, c√¥t√© gauche du graphique), nous commen√ßons avec une image propre et sans bruit. $\alpha_t = 1$. Au fur et √† mesure que nous passons √† des pas de temps plus √©lev√©s, nous nous retrouvons avec presque tout le bruit et $\alpha_t$ chute vers 0.

Lors de l'√©chantillonnage, nous commen√ßons avec du bruit pur au pas de temps $1000$ et nous nous rapprochons lentement du pas de temps $0$. Pour calculer le prochain $t$ de la trajectoire d'√©chantillonnage ($x_{t-1}$ puisque nous passons d'un $t$ √©lev√© √† un $t$ faible), nous pr√©disons le bruit ($\epsilon_\theta(x_t)$, qui est la sortie de notre mod√®le) et nous l'utilisons pour calculer l'image d√©bruit√©e pr√©dite $x_0$. Nous utilisons ensuite cette pr√©diction pour nous d√©placer sur une petite distance dans la "direction pointant vers $x_t$". Enfin, nous pouvons ajouter du bruit suppl√©mentaire √† l'√©chelle de $\sigma_t$. Voici la section de l'article qui montre cette m√©thode en action :

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous disposons donc d'une √©quation permettant de passer de $x_t$ √† $x_{t-1}$, avec une quantit√© de bruit contr√¥lable. Dans notre cas pr√©sent, nous nous int√©ressons plus particuli√®rement au cas o√π nous n'ajoutons aucun bruit suppl√©mentaire, ce qui nous donne un √©chantillonnage DDIM enti√®rement d√©terministe. Voyons ce que cela donne en code :

```py
# Fonction d'√©chantillonnage (DDIM standard)
@torch.no_grad()
def sample(prompt, start_step=0, start_latents=None,
           guidance_scale=3.5, num_inference_steps=30,
           num_images_per_prompt=1, do_classifier_free_guidance=True,
           negative_prompt='', device=device):
  
    # Encoder le prompt
    text_embeddings = pipe._encode_prompt(
            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt
    )

    # Nombre d'√©tapes d'inf√©rence
    pipe.scheduler.set_timesteps(num_inference_steps, device=device)

    # Cr√©er un point de d√©part al√©atoire si nous n'en avons pas d√©j√† un
    if start_latents is None:
        start_latents = torch.randn(1, 4, 64, 64, device=device)
        start_latents *= pipe.scheduler.init_noise_sigma

    latents = start_latents.clone()

    for i in tqdm(range(start_step, num_inference_steps)):
    
        t = pipe.scheduler.timesteps[i]

        # d√©velopper les latents si l'on proc√®de √† un guidage sans classifieur
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # pr√©dire le bruit r√©siduel
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        #r√©aliser un guidage
        if do_classifier_free_guidance:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)


        # Normalement, nous devrions nous fier au planificateur pour g√©rer l'√©tape de mise √† jour :
        # latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample

        # Au lieu de cela, faisons-le nous-m√™mes :
        prev_t = max(1, t.item() - (1000//num_inference_steps)) # t-1
        alpha_t = pipe.scheduler.alphas_cumprod[t.item()]
        alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]
        predicted_x0 = (latents - (1-alpha_t).sqrt()*noise_pred) / alpha_t.sqrt()
        direction_pointing_to_xt = (1-alpha_t_prev).sqrt()*noise_pred
        latents = alpha_t_prev.sqrt()*predicted_x0 + direction_pointing_to_xt

    # Post-traitement
    images = pipe.decode_latents(latents)
    images = pipe.numpy_to_pil(images)

    return images
```

```py
# Tester notre fonction d'√©chantillonnage en g√©n√©rant une image
sample('Watercolor painting of a beach sunset', negative_prompt=negative_prompt, num_inference_steps=50)[0].resize((256, 256))
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Voyez si vous pouvez faire correspondre le code avec l'√©quation de l'article. Notez que $\sigma$=0 puisque nous ne nous int√©ressons qu'au cas o√π il n'y a pas de bruit suppl√©mentaire, nous pouvons donc laisser de c√¥t√© ces √©l√©ments de l'√©quation.

## Inversion

L'objectif est d'inverser le processus d'√©chantillonnage. Nous voulons obtenir un latent bruit√© qui, s'il est utilis√© comme point de d√©part de notre proc√©dure d'√©chantillonnage habituelle, permet de g√©n√©rer l'image originale.

Ici, nous chargeons une image comme image initiale, mais vous pouvez √©galement en g√©n√©rer une vous-m√™me pour l'utiliser √† la place.

```py
# https://www.pexels.com/photo/a-beagle-on-green-grass-field-8306128/
input_image = load_image('https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg', size=(512, 512))
input_image
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous allons √©galement utiliser un prompt pour effectuer l'inversion avec l'aide d'un classifieur libre, alors entrez une description de l'image :

```py
input_image_prompt = "Photograph of a puppy on the grass"
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Ensuite, nous devons transformer cette image PIL en un ensemble de latents que nous utiliserons comme point de d√©part de notre inversion :

```py
# encoder avec le VAE
with torch.no_grad(): latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)
l = 0.18215 * latent.latent_dist.sample()
```

Tr√®s bien, il est temps de passer √† la partie amusante. Cette fonction ressemble √† la fonction d'√©chantillonnage ci-dessus, mais nous nous d√©pla√ßons √† travers les pas de temps dans la direction oppos√©e, en commen√ßant √† $t=0$ et en nous d√©pla√ßant vers un bruit de plus en plus √©lev√©. Et au lieu de mettre √† jour nos latents pour qu'ils soient moins bruyants, nous estimons le bruit pr√©dit et l'utilisons pour ANNULER une √©tape de mise √† jour, en les d√©pla√ßant de $t$ √† $t+1$.

```py
## Inversion
@torch.no_grad()
def invert(start_latents, prompt, guidance_scale=3.5, num_inference_steps=80,
           num_images_per_prompt=1, do_classifier_free_guidance=True,
           negative_prompt='', device=device):
  
    # Encoder le prompt
    text_embeddings = pipe._encode_prompt(
            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt
    )

    # les latents sont maintenant les latents de d√©part sp√©cifi√©s
    latents = start_latents.clone()

    # Nous garderons une liste des latents invers√©s au fur et √† mesure du processus
    intermediate_latents = []

    # D√©finir le nombre d'√©tapes de l'inf√©rence
    pipe.scheduler.set_timesteps(num_inference_steps, device=device)

    # Pas de temps invers√©s <<<<<<<<<<<<<<<<<<<<
    timesteps = reversed(pipe.scheduler.timesteps)

    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps-1):

        # Nous allons sauter l'it√©ration finale
        if i >= num_inference_steps - 1: continue

        t = timesteps[i]

		# d√©velopper les latents si l'on fait de l'orientation sans classifieur
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # pr√©dire le bruit r√©siduel
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # effectuer un guidage
        if do_classifier_free_guidance:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        current_t = max(0, t.item() - (1000//num_inference_steps))#t
        next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
        alpha_t = pipe.scheduler.alphas_cumprod[current_t]
        alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]

        # √âtape de mise √† jour invers√©e (r√©organisation de l'√©tape de mise √† jour pour obtenir x(t) (nouveaux latents) en fonction de x(t-1) (latents actuels)
        latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred


        # Stockage
        intermediate_latents.append(latents)
            
    return torch.cat(intermediate_latents)
```

En l'ex√©cutant sur la repr√©sentation latente de notre photo de chiot, nous obtenons un ensemble de tous les latents interm√©diaires cr√©√©s au cours du processus d'inversion :

```py
inverted_latents = invert(l, input_image_prompt,num_inference_steps=50)
inverted_latents.shape
```
```py
torch.Size([48, 4, 64, 64])
```

Nous pouvons visualiser l'ensemble final de latents - ceux-ci constitueront, nous l'esp√©rons, le point de d√©part bruyant de nos nouvelles tentatives d'√©chantillonnage :

```py
# D√©coder les latents invers√©s finaux
with torch.no_grad():
  im = pipe.decode_latents(inverted_latents[-1].unsqueeze(0))
pipe.numpy_to_pil(im)[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Vous pouvez transmettre ces latents invers√©s au pipeline en utilisant la m√©thode__call__ normale :

```py
pipe(input_image_prompt, latents=inverted_latents[-1][None], num_inference_steps=50, guidance_scale=3.5).images[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Mais c'est l√† que nous voyons notre premier probl√®me : ce n'est pas tout √† fait l'image avec laquelle nous avons commenc√© ! En effet, l'inversion DDIM repose sur une hypoth√®se critique selon laquelle la pr√©diction du bruit √† l'instant $t$ et √† l'instant $t+1$ sera la m√™me, ce qui n'est pas vrai lorsque l'inversion ne porte que sur $50$ ou $100$ pas de temps. Nous pourrions utiliser davantage de pas de temps pour esp√©rer obtenir une inversion plus pr√©cise, mais nous pouvons √©galement tricher et commencer √† partir de, disons, $20/50$ pas d'√©chantillonnage avec les latents interm√©diaires correspondants que nous avons sauvegard√©s lors de l'inversion :
```py
# La raison pour laquelle nous voulons pouvoir sp√©cifier l'√©tape de d√©marrage
start_step=20
sample(input_image_prompt, start_latents=inverted_latents[-(start_step+1)][None], 
       start_step=start_step, num_inference_steps=50)[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Tr√®s proche de notre image d'entr√©e ! Pourquoi faisons-nous cela ? Eh bien, l'espoir est que si nous √©chantillonnons maintenant avec un nouveau prompt, nous obtiendrons une image qui correspond √† l'original SAUF aux endroits pertinents pour le nouveau prompt. Par exemple, en rempla√ßant "puppy" par "cat", nous devrions voir un chat avec un dos et un arri√®re-plan presque identiques :

```py
# √âchantillonnage avec un nouveau prompt
start_step=10
new_prompt = input_image_prompt.replace('puppy', 'cat')
sample(new_prompt, start_latents=inverted_latents[-(start_step+1)][None], 
       start_step=start_step, num_inference_steps=50)[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

### Pourquoi ne pas utiliser img2img ?

Pourquoi s'emb√™ter √† inverser ? Ne peut-on pas simplement ajouter du bruit √† l'image d'entr√©e et la d√©bruiter avec le nouveau prompt ? Nous le pouvons, mais cela entra√Ænera des changements beaucoup plus radicaux partout (si nous ajoutons beaucoup de bruit) ou des changements insuffisants partout (si nous ajoutons moins de bruit). Essayez vous-m√™me :

```py
start_step = 10
num_inference_steps=50
pipe.scheduler.set_timesteps(num_inference_steps)
noisy_l = pipe.scheduler.add_noise(l, torch.randn_like(l), pipe.scheduler.timesteps[start_step])
sample(new_prompt, start_latents=noisy_l, start_step=start_step, num_inference_steps=num_inference_steps)[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Notez la modification beaucoup plus importante de la pelouse et de l'arri√®re-plan.


## Rassembler le tout

Rassemblons le code que nous avons √©crit jusqu'√† pr√©sent dans une fonction simple qui prend une image et deux prompts et effectue une modification en utilisant l'inversion :

```py
def edit(input_image, input_image_prompt, edit_prompt, num_steps=100, start_step=30, guidance_scale=3.5):
    with torch.no_grad(): latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)
    l = 0.18215 * latent.latent_dist.sample()
    inverted_latents = invert(l, input_image_prompt,num_inference_steps=num_steps)
    final_im = sample(edit_prompt, start_latents=inverted_latents[-(start_step+1)][None], 
                      start_step=start_step, num_inference_steps=num_steps, guidance_scale=guidance_scale)[0]
    return final_im
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Et en action :

```py
edit(input_image, 'A puppy on the grass', 'an old grey dog on the grass', num_steps=50, start_step=10)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

```py
edit(input_image, 'A puppy on the grass', 'A blue dog on the lawn', num_steps=50, start_step=12, guidance_scale=6)
```

<Tip> 
‚úèÔ∏è *A votre tour !* essayez ceci sur d'autres images ! Explorez les diff√©rents param√®tres.
</Tip> 
     
## Plus de pas = meilleure performance

Si vous avez des probl√®mes avec des inversions moins pr√©cises, vous pouvez essayer d'utiliser plus de pas (au prix d'un temps d'ex√©cution plus long). Pour tester l'inversion, vous pouvez utiliser notre fonction d'√©dition avec le m√™me prompt :

```py
# Test d'inversion avec beaucoup plus d'√©tapes :
edit(input_image, 'A puppy on the grass', 'A puppy on the grass', num_steps=350, start_step=1)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

C'est beaucoup mieux ! Et en essayant de l'√©diter :

```py
edit(input_image, 'A photograph of a puppy', 'A photograph of a grey cat', num_steps=150, start_step=30, guidance_scale=5.5)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

```py
# source: https://www.pexels.com/photo/girl-taking-photo-1493111/
face = load_image('https://images.pexels.com/photos/1493111/pexels-photo-1493111.jpeg', size=(512, 512))
face
```   

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

```py
edit(face, 'A photograph of a face', 'A photograph of a face with sunglasses', num_steps=250, start_step=30, guidance_scale=3.5)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

```py
edit(face, 'A photograph of a face', 'Acrylic palette knife painting of a face, colorful', num_steps=250, start_step=65, guidance_scale=5.5)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

## Et ensuite ?

Arm√© des connaissances de ce *notebook*, nous vous recommandons d'√©tudier [*Null-text Inversion*](https://null-text-inversion.github.io/) qui s'appuie sur DDIM en optimisant le texte nul (prompt inconditionnel) lors de l'inversion pour des inversions plus pr√©cises et de meilleures √©ditions.