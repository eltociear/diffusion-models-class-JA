# Diffusion pour l'audio

<CourseFloatingBanner unit={4}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
	{label: "Diffusion pour l'audio", value: "https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/units/fr/unit4/diffusion_for_audio.ipynb"},
    {label: "Diffusion pour l'audio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/units/fr/unit4/diffusion_for_audio.ipynb"},

]} />

Dans ce *notebook*, nous allons jeter un bref coup d'≈ìil √† la g√©n√©ration d'audio avec des mod√®les de diffusion.
Ce que vous allez apprendre :
- Comment l'audio est repr√©sent√© dans un ordinateur
- Les m√©thodes de conversion entre les donn√©es audio brutes et les spectrogrammes
- Comment pr√©parer un chargeur de donn√©es avec une fonction personnalis√©e pour convertir des tranches d'audio en spectrogrammes
- *Finetuner* un mod√®le de diffusion audio existant sur un genre de musique sp√©cifique
- T√©l√©charger votre pipeline personnalis√© sur le Hub d'Hugging Face

Mise en garde : il s'agit principalement d'un objectif p√©dagogique - rien ne garantit que notre mod√®le sonnera bien üòâ

Commen√ßons !

## Configuration et importations
```py
# !pip install -q datasets diffusers torchaudio accelerate
```     
```py
import torch, random
import numpy as np
import torch.nn.functional as F
from tqdm.auto import tqdm
from IPython.display import Audio
from matplotlib import pyplot as plt
from diffusers import DiffusionPipeline
from torchaudio import transforms as AT
from torchvision import transforms as IT
```

## Echantillonnage √† partir d'un pipeline audio pr√©-entra√Æn√©

Commen√ßons par suivre la [documentation](https://huggingface.co/docs/diffusers/api/pipelines/audio_diffusion) pour charger un mod√®le de diffusion audio pr√©existant :

```py
# Chargement d'un pipeline de diffusion audio pr√©-entra√Æn√©
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = DiffusionPipeline.from_pretrained("teticio/audio-diffusion-instrumental-hiphop-256").to(device)
```

Comme pour les pipelines que nous avons utilis√©s dans les unit√©s pr√©c√©dentes, nous pouvons cr√©er des √©chantillons en appelant le pipeline comme suit :

```py
# √âchantillonner √† partir du pipeline et afficher les r√©sultats
output = pipe()
display(output.images[0])
display(Audio(output.audios[0], rate=pipe.mel.get_sample_rate()))
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Ici, l'argument `rate` sp√©cifie la fr√©quence d'√©chantillonnage de l'audio ; nous y reviendrons plus tard. Vous remarquerez √©galement que le pipeline renvoie plusieurs choses. Que se passe-t-il ici ? Examinons de plus pr√®s les deux sorties.

La premi√®re est un tableau de donn√©es, repr√©sentant l'audio g√©n√©r√© :

```py
# Le tableau audio :
output.audios[0].shape
```
```py
(1, 130560)
```

La seconde ressemble √† une image en niveaux de gris :

```py
# L'image de sortie (spectrogramme)
output.images[0].size
```     
```py
(256, 256)
```

Cela nous donne un aper√ßu du fonctionnement de ce pipeline. L'audio n'est pas directement g√©n√©r√© par diffusion. Au lieu de cela, le pipeline a le m√™me type d'UNet 2D que les pipelines de g√©n√©ration d'images inconditionnelles que nous avons vus dans l'unit√© 1, qui est utilis√© pour g√©n√©rer le spectrogramme, qui est ensuite post-trait√© dans l'audio final.

Le pipeline poss√®de un composant suppl√©mentaire qui g√®re ces conversions, auquel nous pouvons acc√©der via `pipe.mel` :

```py
pipe.mel
```
```py
Mel {
  "_class_name": "Mel",
  "_diffusers_version": "0.12.0.dev0",
  "hop_length": 512,
  "n_fft": 2048,
  "n_iter": 32,
  "sample_rate": 22050,
  "top_db": 80,
  "x_res": 256,
  "y_res": 256
}
```

## De l'audio √† l'image et inversement

Une "forme d'onde" encode les √©chantillons audio bruts dans le temps. Il peut s'agir du signal √©lectrique re√ßu d'un microphone, par exemple. Travailler avec cette repr√©sentation du "domaine temporel" peut s'av√©rer d√©licat, c'est pourquoi il est courant de la convertir sous une autre forme, commun√©ment appel√©e spectrogramme. Un spectrogramme montre l'intensit√© de diff√©rentes fr√©quences (axe y) en fonction du temps (axe x) :

```py
# Calculer et afficher un spectrogramme pour notre √©chantillon audio g√©n√©r√© en utilisant torchaudio
spec_transform = AT.Spectrogram(power=2)
spectrogram = spec_transform(torch.tensor(output.audios[0]))
print(spectrogram.min(), spectrogram.max())
log_spectrogram = spectrogram.log()
plt.imshow(log_spectrogram[0], cmap='gray');
```    
```py
tensor(0.) tensor(6.0842)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>


Le spectrogramme que nous venons de cr√©er contient des valeurs comprises entre 0,0000000000001 et 1, la plupart d'entre elles √©tant proches de la limite inf√©rieure de cette plage. Ce n'est pas l'id√©al pour la visualisation ou la mod√©lisation. En fait, nous avons d√ª prendre le logarithme de ces valeurs pour obtenir un trac√© en niveaux de gris qui montre des d√©tails. Pour cette raison, nous utilisons g√©n√©ralement un type sp√©cial de spectrogramme appel√© Mel spectrogramme, qui est con√ßu pour capturer les types d'informations qui sont importantes pour l'audition humaine en appliquant certaines transformations aux diff√©rentes composantes de fr√©quence du signal.

Quelques transformations audio de la documentation [torchaudio](https://pytorch.org/audio/stable/transforms.html)

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Heureusement pour nous, nous n'avons pas besoin de nous pr√©occuper de ces transformations, la fonctionnalit√© mel du pipeline s'occupe de ces d√©tails pour nous. En l'utilisant, nous pouvons convertir une image de spectrogramme en audio comme suit :

```py
a = pipe.mel.image_to_audio(output.images[0])
a.shape
```
```py
(130560,)
```

Nous pouvons √©galement convertir un tableau de donn√©es audio en images de spectrogramme en chargeant d'abord les donn√©es audio brutes, puis en appelant la fonction audio_slice_to_image(). Les clips plus longs sont automatiquement d√©coup√©s en morceaux de la bonne longueur pour produire une image de spectrogramme de 256x256 :

```py
pipe.mel.load_audio(raw_audio=a)
im = pipe.mel.audio_slice_to_image(0)
im
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

L'audio est repr√©sent√© sous la forme d'un long tableau de nombres. Pour l'√©couter nous avons besoin d'une autre information cl√© : la fr√©quence d'√©chantillonnage. Combien d'√©chantillons (valeurs individuelles) utilisons-nous pour repr√©senter une seconde d'audio ?

Nous pouvons voir la fr√©quence d'√©chantillonnage utilis√©e lors de l'entra√Ænement de ce pipeline avec :

```py
sample_rate_pipeline = pipe.mel.get_sample_rate()
sample_rate_pipeline
```
```py
22050
```

Si nous sp√©cifions mal la fr√©quence d'√©chantillonnage, nous obtenons un son acc√©l√©r√© ou ralenti :

```py
display(Audio(output.audios[0], rate=44100)) # Vitesse x2
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

## *Finetuning* du pipeline

Maintenant que nous avons une compr√©hension approximative du fonctionnement du pipeline, nous allons le *finetuner* sur de nouvelles donn√©es audio !

Le jeu de donn√©es est une collection de clips audio de diff√©rents genres, que nous pouvons charger depuis le Hub de la mani√®re suivante :

```py
from datasets import load_dataset
dataset = load_dataset('lewtun/music_genres', split='train')
dataset
```
```py
Dataset({
    features: ['audio', 'song_id', 'genre_id', 'genre'],
    num_rows: 19909
})
```

Vous pouvez utiliser le code ci-dessous pour voir les diff√©rents genres dans le jeu de donn√©es et combien d'√©chantillons sont contenus dans chacun d'eux :

```py
for g in list(set(dataset['genre'])):
  print(g, sum(x==g for x in dataset['genre']))
```
```py
Pop 945
Blues 58
Punk 2582
Old-Time / Historic 408
Experimental 1800
Folk 1214
Electronic 3071
Spoken 94
Classical 495
Country 142
Instrumental 1044
Chiptune / Glitch 1181
International 814
Ambient Electronic 796
Jazz 306
Soul-RnB 94
Hip-Hop 1757
Easy Listening 13
Rock 3095
```

Le jeu de donn√©es contient les donn√©es audio sous forme de tableaux :

```py
audio_array = dataset[0]['audio']['array']
sample_rate_dataset = dataset[0]['audio']['sampling_rate']
print('Audio array shape:', audio_array.shape)
print('Sample rate:', sample_rate_dataset)
display(Audio(audio_array, rate=sample_rate_dataset))
```
```py
Audio array shape: (1323119,)
Sample rate: 44100
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Notez que la fr√©quence d'√©chantillonnage de cet audio est plus √©lev√©e. Si nous voulons utiliser le pipeline existant, nous devrons le "r√©√©chantillonner" pour qu'il corresponde √† la fr√©quence d'√©chantillonnage. Les clips sont √©galement plus longs que ceux pour lesquels le pipeline est configur√©. Heureusement, lorsque nous chargeons l'audio √† l'aide de pipe.mel, il d√©coupe automatiquement le clip en sections plus petites :

```py
a = dataset[0]['audio']['array'] # Obtenir le tableau audio
pipe.mel.load_audio(raw_audio=a) # Le charger avec pipe.mel
pipe.mel.audio_slice_to_image(0) # Visualiser la premi√®re "tranche" sous forme de spectrogramme
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Nous devons penser √† ajuster le taux d'√©chantillonnage, car les donn√©es de ce jeu de donn√©es comportent deux fois plus d'√©chantillons par seconde :
```py
sample_rate_dataset = dataset[0]['audio']['sampling_rate']
sample_rate_dataset
```
```py
44100
```

Ici, nous utilisons les transformations de torchaudio (import√©es sous le nom AT) pour effectuer le r√©√©chantillonnage, le pipeline mel pour transformer l'audio en image et les transformations de torchvision (import√©es sous le nom IT) pour transformer les images en tenseurs. Nous obtenons ainsi une fonction qui transforme un clip audio en un tenseur de spectrogramme que nous pouvons utiliser pour nous entra√Æner :

```py
resampler = AT.Resample(sample_rate_dataset, sample_rate_pipeline, dtype=torch.float32)
to_t = IT.ToTensor()

def to_image(audio_array):
  audio_tensor = torch.tensor(audio_array).to(torch.float32)
  audio_tensor = resampler(audio_tensor)
  pipe.mel.load_audio(raw_audio=np.array(audio_tensor))
  num_slices = pipe.mel.get_number_of_slices()
  slice_idx = random.randint(0, num_slices-1) # Piocher une tranche al√©atoire √† chaque fois (√† l'exception de la derni√®re tranche courte)
  im = pipe.mel.audio_slice_to_image(slice_idx) 
  return im
```

Nous utiliserons notre fonction to_image() dans le cadre d'une fonction collate personnalis√©e pour transformer notre jeu de donn√©es en un chargeur de donn√©es utilisable pour l'entra√Ænement. La fonction collate d√©finit la mani√®re de transformer un batch d'exemples du jeu de donn√©es en un batch final de donn√©es pr√™tes √† √™tre entra√Æn√©es. Dans ce cas, nous transformons chaque √©chantillon audio en une image de spectrogramme et nous empilons les tenseurs r√©sultants :

```py
def collate_fn(examples):
  # vers l'image -> vers le tenseur -> redimensionnement vers (-1, 1) -> empiler dans le batch
  audio_ims = [to_t(to_image(x['audio']['array']))*2-1 for x in examples]
  return torch.stack(audio_ims)

# Cr√©er un jeu de donn√©es avec uniquement le genre de chansons 'Chiptune / Glitch'
batch_size=4 # 4 sur Colab, 12 sur A100
chosen_genre = 'Electronic' # <<< Essayer d'entra√Æner sur des genres diff√©rents <<<
indexes = [i for i, g in enumerate(dataset['genre']) if g == chosen_genre]
filtered_dataset = dataset.select(indexes)
dl = torch.utils.data.DataLoader(filtered_dataset.shuffle(), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)
batch = next(iter(dl))
print(batch.shape)
```
```py
torch.Size([4, 1, 256, 256])
```

**NB : Vous devrez utiliser une taille de batch inf√©rieure (par exemple 4) √† moins que vous ne disposiez d'une grande quantit√© de vRAM GPU.

## Boucle d'entra√Ænement

Voici une boucle d'entra√Ænement simple qui s'ex√©cute √† travers le chargeur de donn√©es pour quelques √©poques afin de *finetuner* le pipeline UNet. Vous pouvez √©galement ignorer cette cellule et charger le pipeline avec le code de la cellule suivante.

```py
epochs = 3
lr = 1e-4

pipe.unet.train()
pipe.scheduler.set_timesteps(1000)
optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=lr)

for epoch in range(epochs):
    for step, batch in tqdm(enumerate(dl), total=len(dl)):
        
        # Pr√©parer les images d'entr√©e
        clean_images = batch.to(device)
        bs = clean_images.shape[0]

        # √âchantillonner un pas de temps al√©atoire pour chaque image
        timesteps = torch.randint(
            0, pipe.scheduler.num_train_timesteps, (bs,), device=clean_images.device
        ).long()

        # Ajouter du bruit aux images propres en fonction de l'ampleur du bruit √† chaque √©tape
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        noisy_images = pipe.scheduler.add_noise(clean_images, noise, timesteps)

        # Obtenir la pr√©diction du mod√®le
        noise_pred = pipe.unet(noisy_images, timesteps, return_dict=False)[0]

        # Calculer la perte
        loss = F.mse_loss(noise_pred, noise)
        loss.backward(loss)

        # Mise √† jour des param√®tres du mod√®le √† l'aide de l'optimiseur
        optimizer.step()
        optimizer.zero_grad()
```

```py
# OU : Charger la version entra√Æn√©e pr√©c√©demment
pipe = DiffusionPipeline.from_pretrained("johnowhitaker/Electronic_test").to(device)
```

```py
output = pipe()
display(output.images[0])
display(Audio(output.audios[0], rate=22050))
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

```py
# Cr√©er un √©chantillon plus long en passant un tenseur de bruit de d√©part avec une forme diff√©rente
noise = torch.randn(1, 1, pipe.unet.sample_size[0],pipe.unet.sample_size[1]*4).to(device)
output = pipe(noise=noise)
display(output.images[0])
display(Audio(output.audios[0], rate=22050))
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Ce ne sont pas les r√©sultats les plus impressionnants mais c'est un d√©but :) Essayez d'ajuster le taux d'apprentissage et le nombre d'√©poques, et partagez vos meilleurs r√©sultats sur Discord pour que nous puissions nous am√©liorer ensemble !

Quelques √©l√©ments √† prendre en compte
- Nous travaillons avec des images de spectrogrammes carr√©s de 256 pixels ce qui limite la taille de nos batchs. Pouvez-vous r√©cup√©rer de l'audio de qualit√© suffisante √† partir d'un spectrogramme de 128x128 ?
- Au lieu d'une augmentation al√©atoire de l'image, nous choisissons √† chaque fois des tranches diff√©rentes du clip audio, mais cela pourrait-il √™tre am√©lior√© avec diff√©rents types d'augmentation lorsque l'on s'entra√Æne pendant de nombreuses √©poques ?
- Comment pourrions-nous utiliser cette m√©thode pour g√©n√©rer des clips plus longs ? Peut-√™tre pourriez-vous g√©n√©rer un clip de d√©part de 5 secondes, puis utiliser des id√©es inspir√©es de la compl√©tion d'images (*inpainting*) pour continuer √† g√©n√©rer des segments audio suppl√©mentaires √† partir du clip initial...
- Quel est l'√©quivalent d'une image √† image dans ce contexte de diffusion de spectrogrammes ?

## Pousser sur le Hub

Une fois que vous √™tes satisfait de votre mod√®le, vous pouvez le sauvegarder et le transf√©rer sur le Hub pour que d'autres personnes puissent en profiter :

```py
from huggingface_hub import get_full_repo_name, HfApi, create_repo, ModelCard
```   

```py
# Choisir un nom pour le mod√®le
model_name = "audio-diffusion-electronic"
hub_model_id = get_full_repo_name(model_name)
```

```py
# Sauvegarder le pipeline localement
pipe.save_pretrained(model_name)
```

```py
# Inspecter le contenu du dossier
!ls {model_name}
```
```py
mel  model_index.json  scheduler  unet
```

```py
# Cr√©er un d√©p√¥t
create_repo(hub_model_id)
```

```py
# T√©l√©charger les fichiers
api = HfApi()
api.upload_folder(
    folder_path=f"{model_name}/scheduler", path_in_repo="scheduler", repo_id=hub_model_id
)
api.upload_folder(
    folder_path=f"{model_name}/mel", path_in_repo="mel", repo_id=hub_model_id
)
api.upload_folder(folder_path=f"{model_name}/unet", path_in_repo="unet", repo_id=hub_model_id)
api.upload_file(
    path_or_fileobj=f"{model_name}/model_index.json",
    path_in_repo="model_index.json",
    repo_id=hub_model_id,
)
```

```py
# Pousser une carte de mod√®le
content = f"""
---
license: mit
tags:
- pytorch
- diffusers
- unconditional-audio-generation
- diffusion-models-class
---

# Model Card for Unit 4 of the [Diffusion Models Class üß®](https://github.com/huggingface/diffusion-models-class)

This model is a diffusion model for unconditional audio generation of music in the genre {chosen_genre}

## Usage

```python
from IPython.display import Audio
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("{hub_model_id}")
output = pipe()
display(output.images[0])
display(Audio(output.audios[0], rate=pipe.mel.get_sample_rate()))
```
"""

card = ModelCard(content)
card.push_to_hub(hub_model_id)
``` 

## Conclusion

Ce *notebook* vous a donn√©, nous l'esp√©rons, un petit aper√ßu du potentiel de la g√©n√©ration audio. Consultez certaines des r√©f√©rences li√©es √† la vue d'ensemble de cette unit√© pour voir des m√©thodes plus fantaisistes et des √©chantillons stup√©fiants qu'elles peuvent cr√©er !