# Vue d'ensemble

<CourseFloatingBanner
    unit={2}
    classNames="absolute z-10 right-0 top-0"
/>

Dans cette unit√©, vous apprendrez √† utiliser et √† adapter les mod√®les de diffusion pr√©-entra√Æn√©s de nouvelles fa√ßons. Vous verrez √©galement comment nous pouvons cr√©er des mod√®les de diffusion qui prennent des entr√©es suppl√©mentaires comme **conditionnement** pour contr√¥ler le processus de g√©n√©ration.

## Vue d'ensemble de cette unit√© :rocket:

Les diff√©rentes √©tapes √† suivre pour cette unit√© :

- Lisez le mat√©riel ci-dessous pour avoir une vue d'ensemble des id√©es cl√©s de cette unit√©
- Consultez le *notebook _**Finetuning et guidage**_ pour finetuner un mod√®le de diffusion existant sur un nouveau jeu de donn√©es en utilisant la biblioth√®que ü§ó *Diffusers* et pour modifier la proc√©dure d'√©chantillonnage en utilisant le guidage.
- Suivez l'exemple dans le *notebook* pour partager une d√©mo Gradio pour votre mod√®le personnalis√©
- (Facultatif) Consultez le *notebook _**Mod√®le de diffusion conditionn√© par la classe**_ pour voir comment nous pouvons ajouter un contr√¥le suppl√©mentaire au processus de g√©n√©ration
- (Facultatif) Regardez [cette vid√©o](https://www.youtube.com/watch?v=mY20iKOQ2zw) (en anglais) pour une pr√©sentation informelle du mat√©riel de cette unit√©
 
## *Finetuning*

Comme vous avez pu le constater dans l'unit√© 1, entra√Æner des mod√®les de diffusion √† partir de z√©ro peut prendre beaucoup de temps ! Le temps et les donn√©es n√©cessaires pour entra√Æner un mod√®le √† partir de z√©ro peuvent devenir irr√©alisables, en particulier lorsque l'on passe √† des r√©solutions plus √©lev√©es. Heureusement, il existe une solution : commencer par un mod√®le qui a d√©j√† √©t√© entra√Æn√© ! Ainsi, nous partons d'un mod√®le qui a d√©j√† appris √† d√©bruiter des images, et nous esp√©rons que cela constituera un meilleur point de d√©part qu'un mod√®le initialis√© de mani√®re al√©atoire.

![Example images generated with a model trained on LSUN Bedrooms and fine-tuned for 500 steps on WikiArt](https://api.wandb.ai/files/johnowhitaker/dm_finetune/2upaa341/media/images/Sample%20generations_501_d980e7fe082aec0dfc49.png)

Le *finetuning* fonctionne g√©n√©ralement mieux si les nouvelles donn√©es ressemblent quelque peu aux donn√©es d'entra√Ænement originales du mod√®le de base (par exemple, commencer avec un mod√®le entra√Æn√© sur les visages est probablement une bonne id√©e si vous essayez de g√©n√©rer des visages de dessins anim√©s), mais il est surprenant de constater que les avantages persistent m√™me si le domaine est modifi√© de mani√®re assez radicale. L'image ci-dessus est g√©n√©r√©e √† partir d'un [mod√®le entra√Æn√© sur le jeu de donn√©es LSUN Bedrooms](https://huggingface.co/google/ddpm-bedroom-256) et *finetun√©* sur 500 √©tapes sur [le jeu de donn√©es WikiArt](https://huggingface.co/datasets/huggan/wikiart). Le [script d'entra√Ænement](https://github.com/huggingface/diffusion-models-class/blob/main/unit2/finetune_model.py) est inclus √† titre de r√©f√©rence dans les *notebooks* de cette unit√©.

## Guidage
Les mod√®les inconditionnels ne donnent pas beaucoup de contr√¥le sur ce qui est g√©n√©r√©. Nous pouvons entra√Æner un mod√®le conditionnel (plus d'informations √† ce sujet dans la section suivante) qui prend des entr√©es suppl√©mentaires pour aider √† diriger le processus de g√©n√©ration, mais que faire si nous avons d√©j√† entra√Æn√© un mod√®le inconditionnel que nous aimerions utiliser ? C'est l√† qu'intervient le guidage, un processus par lequel les pr√©dictions du mod√®le √† chaque √©tape du processus de g√©n√©ration sont √©valu√©es par rapport √† une fonction de guidage et modifi√©es de mani√®re √† ce que l'image finale g√©n√©r√©e corresponde mieux √† nos attentes. 

![guidance example image](guidance_eg.png)

Cette fonction de guidage peut √™tre presque n'importe quoi, ce qui en fait une technique puissante ! Dans le *notebook*, nous partons d'un exemple simple (contr√¥ler la couleur, comme illustr√© dans l'exemple de sortie ci-dessus) pour arriver √† un exemple utilisant un puissant mod√®le pr√©-entra√Æn√© appel√© CLIP qui nous permet de guider la g√©n√©ration sur la base d'une description textuelle. 

## Conditionnement

Le guidage est un excellent moyen d'exploiter davantage un mod√®le de diffusion inconditionnel, mais si nous disposons d'informations suppl√©mentaires (telles qu'une √©tiquette de classe ou une l√©gende d'image) pendant l'entra√Ænement, nous pouvons √©galement les transmettre au mod√®le afin qu'il les utilise pour √©tablir ses pr√©dictions. Ce faisant, nous cr√©ons un mod√®le **conditionnel**, que nous pouvons contr√¥ler au moment de l'inf√©rence en contr√¥lant ce qui est fourni comme conditionnement. Le *notebook* montre un exemple de mod√®le conditionn√© par une classe qui apprend √† g√©n√©rer des images en fonction d'une √©tiquette de classe. 

![conditioning example](conditional_digit_generation.png)

Il existe un certain nombre de fa√ßons de transmettre ces informations de conditionnement, par exemple
- En les introduisant sous forme de canaux suppl√©mentaires dans l'entr√©e du UNet. Cette m√©thode est souvent utilis√©e lorsque l'information de conditionnement a la m√™me forme que l'image, comme un masque de segmentation, une carte de profondeur ou une version floue de l'image (dans le cas d'un mod√®le de restauration/superr√©solution). Cela fonctionne aussi pour d'autres types de conditionnement. Par exemple, dans le *notebook*, l'√©tiquette de la classe est associ√©e en avec un ench√¢ssement puis √©tendue pour avoir la m√™me largeur et la m√™me hauteur que l'image d'entr√©e, de sorte qu'elle puisse √™tre introduite sous forme de canaux suppl√©mentaires.
- La cr√©ation d'un ench√¢ssement et sa projection √† une taille correspondant au nombre de canaux √† la sortie d'une ou de plusieurs couches internes du UNet, puis son ajout √† ces sorties. C'est ainsi que le conditionnement du pas de temps est g√©r√©, par exemple. La sortie de chaque bloc Resnet est compl√©t√©e par une projection de l'ench√¢ssement du pas de temps. Ceci est utile lorsque vous avez un vecteur tel qu'une l'ench√¢ssement CLIP comme information de conditionnement. Un exemple notable est ['Image Variations' version of Stable Diffusion](https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations) qui fait exactement cela.
- L'ajout de couches d'attention crois√©e qui peuvent s'occuper d'une s√©quence transmise en tant que conditionnement. Ceci est particuli√®rement utile lorsque le conditionnement se pr√©sente sous la forme d'un texte. Le texte est mis en correspondance avec une s√©quence d'ench√¢ssements √† l'aide d'un *transformer* puis les couches d'attention crois√©e du UNet sont utilis√©es pour incorporer cette information dans le chemin de d√©bruitage. Nous verrons cela en action dans l'unit√© 3 lorsque nous examinerons comment Stable Diffusion g√®re le conditionnement du texte.


## Notebooks

| Chapitre                                    | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |
|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Finetuning et guidage                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_finetuning_and_guidance.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit2/_finetuning_and_guidance.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_finetuning_and_guidance.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_finetuning_and_guidance.ipynb)              |
| Mod√®le de diffusion conditionn√© par la classe  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_class_conditioned_diffusion_model_example.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/fr/unit2/_class_conditioned_diffusion_model_example.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_class_conditioned_diffusion_model_example.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/fr/unit2/_class_conditioned_diffusion_model_example.ipynb)              |


La plus grande partie du mat√©riel se trouve dans _**Finetuning et guidage**_, o√π nous explorons ces deux sujets √† travers des exemples travaill√©s. Le *notebook* montre comment vous pouvez *finetuner* un mod√®le existant sur de nouvelles donn√©es, ajouter des conseils, et partager le r√©sultat sous forme de d√©mo Gradio. Il y a un script d'accompagnement ([finetune_model.py](https://github.com/huggingface/diffusion-models-class/blob/main/unit2/finetune_model.py)) qui facilite l'exp√©rimentation de diff√©rents param√®tres de *finetuning*, et [un [Space](https://huggingface.co/spaces/johnowhitaker/color-guided-wikiart-diffusion) que vous pouvez utiliser comme patron pour partager votre propre d√©mo sur ü§ó Spaces. 

Dans le notebook _**Mod√®le de diffusion conditionn√© par la classe**_, nous montrons un bref exemple de cr√©ation d'un mod√®le de diffusion conditionn√© par les √©tiquettes de classe √† l'aide du jeu de donn√©es MNIST. L'objectif est de d√©montrer l'id√©e principale aussi simplement que possible : en donnant au mod√®le des informations suppl√©mentaires sur ce qu'il est cens√© d√©bruiter, nous pouvons contr√¥ler ult√©rieurement les types d'images g√©n√©r√©es au moment de l'inf√©rence.

## Project

Following the examples in the _**Fine-tuning and Guidance**_ notebook, fine-tune your own model or pick an existing model and create a Gradio demo to showcase your new guidance skills. Don't forget to share your demo on Discord, Twitter etc so we can admire your work!

## Ressources compl√©mentaires
Une liste non exhaustive de ressources (en anglais) √† consulter :
- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) est une introduction de la m√©thode d'√©chantillonnage DDIM (utilis√©e par DDIMScheduler)
- [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) est une introduction de m√©thodes pour conditionner les mod√®les de diffusion sur le texte
- [eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers](https://arxiv.org/abs/2211.01324) montre comment diff√©rents types de conditionnement peuvent √™tre utilis√©s ensemble pour contr√¥ler encore davantage les types d'√©chantillons g√©n√©r√©s

Vous avez identifi√© d'autres ressources int√©ressantes ? Faites-le nous savoir et nous les ajouterons √† cette liste.