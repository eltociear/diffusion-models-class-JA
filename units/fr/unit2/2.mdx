# Finetuning et guidage

<CourseFloatingBanner unit={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Finetuning et guidage", value: "https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/units/fr/unit2/finetuning_and_guidance.ipynb""},
    {label: "Finetuning et guidage", value: "https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/units/fr/unit2/finetuning_and_guidance.ipynb"},
]} />


Dans ce *notebook*, nous allons couvrir deux approches principales pour adapter les mod√®les de diffusion existants :

* Avec le *finetuning*, nous r√©entra√Ænerons les mod√®les existants sur de nouvelles donn√©es afin de modifier le type de r√©sultats qu'ils produisent.
* Avec le **guidage**, nous prenons un mod√®le existant et dirigeons le processus de g√©n√©ration au moment de l'inf√©rence pour un contr√¥le suppl√©mentaire.

## Ce que vous apprendrez :

A la fin de ce *notebook*, vous saurez comment :

- Cr√©er une boucle d'√©chantillonnage et g√©n√©rer des √©chantillons plus rapidement √† l'aide d'un nouveau planificateur
- *Finetuner* un mod√®le de diffusion existant sur de nouvelles donn√©es, y compris :
  - Utiliser l'accumulation du gradient pour contourner certains des probl√®mes li√©s aux petits batchs.
  - Enregistrer les √©chantillons dans [Weights and Biases] (https://wandb.ai/site) pendant l'entra√Ænement pour suivre la progression (via le script d'exemple joint).
  - Sauvegarder le pipeline r√©sultant et le t√©l√©charger sur le Hub
- Guider le processus d'√©chantillonnage avec des fonctions de perte suppl√©mentaires pour ajouter un contr√¥le sur les mod√®les existants, y compris :
  - Explorer diff√©rentes approches de guidage avec une simple perte bas√©e sur la couleur
  - Utiliser CLIP pour guider la g√©n√©ration √† l'aide d'un prompt de texte
  - Partager une boucle d'√©chantillonnage personnalis√©e en utilisant Gradio et ü§ó Spaces.


## Configuration et importations

Pour enregistrer vos mod√®les *finetun√©s* sur le Hub d'Hugging Face, vous devrez vous connecter avec un *token* qui a un acc√®s en √©criture. Le code ci-dessous vous invite √† le faire et vous renvoie √† la page des *tokens* de votre compte. Vous aurez √©galement besoin d'un compte Weights and Biases si vous souhaitez utiliser le script d'entra√Ænement pour enregistrer des √©chantillons au fur et √† mesure que le mod√®le s'entra√Æne. L√† encore, le code devrait vous inviter √† vous connecter l√† o√π c'est n√©cessaire.

A part cela, la seule chose √† faire est d'installer quelques d√©pendances, d'importer tout ce dont nous aurons besoin et de sp√©cifier l'appareil que nous utiliserons :

```py
!pip install -qq diffusers datasets accelerate wandb open-clip-torch
``` 

```py
# Code pour se connecter au Hub d'Hugging Face, n√©cessaire pour partager les mod√®les
# Assurez-vous d'utiliser un *token* avec un acc√®s WRITE (√©criture)
from huggingface_hub import notebook_login

notebook_login()
```    
```py  
Token is valid.
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /root/.huggingface/token
Login successful
```  
```py
import numpy as np
import torch
import torch.nn.functional as F
import torchvision
from datasets import load_dataset
from diffusers import DDIMScheduler, DDPMPipeline
from matplotlib import pyplot as plt
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
``` 
```py
device = (
    "mps"
    if torch.backends.mps.is_available()
    else "cuda"
    if torch.cuda.is_available()
    else "cpu"
)
```
    
## Chargement d'un pipeline pr√©-entra√Æn√©

Pour commencer ce *notebook*, chargeons un pipeline existant et voyons ce que nous pouvons en faire :

```py
image_pipe = DDPMPipeline.from_pretrained("google/ddpm-celebahq-256")
image_pipe.to(device);
```

La g√©n√©ration d'images est aussi simple que l'ex√©cution de la m√©thode [`__call__`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/ddpm/pipeline_ddpm.py#L42) du pipeline en l'appelant comme une fonction :

```py
images = image_pipe().images
images[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Sympathique, mais LENT ! Avant d'aborder les sujets principaux du jour, jetons un coup d'≈ìil √† la boucle d'√©chantillonnage proprement dite et voyons comment nous pouvons utiliser un √©chantillonneur plus sophistiqu√© pour l'acc√©l√©rer.

## √âchantillonnage plus rapide avec DDIM

√Ä chaque √©tape, le mod√®le est nourri d'une entr√©e bruyante et il lui est demand√© de pr√©dire le bruit (et donc une estimation de ce √† quoi l'image enti√®rement d√©bruit√©e pourrait ressembler). Au d√©part, ces pr√©dictions ne sont pas tr√®s bonnes, c'est pourquoi nous d√©composons le processus en plusieurs √©tapes. Cependant, l'utilisation de plus de 1000 √©tapes s'est av√©r√©e inutile, et une multitude de recherches r√©centes ont explor√© la mani√®re d'obtenir de bons √©chantillons avec le moins d'√©tapes possible.

Dans la biblioth√®que ü§ó *Diffusers*, ces m√©thodes d'√©chantillonnage sont g√©r√©es par un planificateur, qui doit effectuer chaque mise √† jour via la fonction `step()`. Pour g√©n√©rer une image, on commence par un bruit al√©atoire $x$. Ensuite, pour chaque pas de temps dans le planificateur de bruit, nous introduisons l'entr√©e bruit√©e $x$ dans le mod√®le et transmettons la pr√©diction r√©sultante √† la fonction `step()`. Celle-ci renvoie une sortie avec un attribut `prev_sample`.  "previous" parce que nous revenons en arri√®re dans le temps, d'un niveau de bruit √©lev√© √† un niveau de bruit faible (√† l'inverse du processus de diffusion vers l'avant).

Voyons cela en action ! Tout d'abord, nous chargeons un planificateur, ici un `DDIMScheduler` bas√© sur le papier [*Denoising Diffusion Implicit Models*](https://arxiv.org/abs/2010.02502) qui peut donner des √©chantillons d√©cents en beaucoup moins d'√©tapes que l'impl√©mentation originale du DDPM :

```py
# Cr√©er un nouveau planificateur et d√©finir le nombre d'√©tapes d'inf√©rence
scheduler = DDIMScheduler.from_pretrained("google/ddpm-celebahq-256")
scheduler.set_timesteps(num_inference_steps=40)
```  

Vous pouvez constater que ce mod√®le effectue 40 √©tapes au total, chaque saut √©quivalant √† 25 √©tapes du programme original de 1000 √©tapes :

```py
scheduler.timesteps
```  
```py
tensor([975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 725, 700, 675, 650,
        625, 600, 575, 550, 525, 500, 475, 450, 425, 400, 375, 350, 325, 300,
        275, 250, 225, 200, 175, 150, 125, 100,  75,  50,  25,   0])
```

Cr√©ons 4 images al√©atoires et ex√©cutons la boucle d'√©chantillonnage, en visualisant √† la fois le $x$ actuel et la version d√©bruit√©e pr√©dite au fur et √† mesure de l'avancement du processus :

```py
# Le point de d√©part al√©atoire
x = torch.randn(4, 3, 256, 256).to(device)  # Batch de 4 images √† 3 canaux de 256 x 256 px

# Boucle sur les pas de temps d'√©chantillonnage
for i, t in tqdm(enumerate(scheduler.timesteps)):

    # Pr√©parer l'entr√©e du mod√®le
    model_input = scheduler.scale_model_input(x, t)

    # Obtenir la pr√©diction
    with torch.no_grad():
        noise_pred = image_pipe.unet(model_input, t)["sample"]

    # Calculer la forme que devrait prendre l'√©chantillon mis √† jour √† l'aide du planificateur
    scheduler_output = scheduler.step(noise_pred, t, x)

    # Mise √† jour de x
    x = scheduler_output.prev_sample

    # Occasionnellement, afficher √† la fois x et les images d√©bruit√©es pr√©dites
    if i % 10 == 0 or i == len(scheduler.timesteps) - 1:
        fig, axs = plt.subplots(1, 2, figsize=(12, 5))

        grid = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0)
        axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)
        axs[0].set_title(f"Current x (step {i})")

        pred_x0 = (
            scheduler_output.pred_original_sample
        )  # Non disponible pour tous les planificateurs
        grid = torchvision.utils.make_grid(pred_x0, nrow=4).permute(1, 2, 0)
        axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)
        axs[1].set_title(f"Predicted denoised images (step {i})")
        plt.show()
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Comme vous pouvez le voir, les pr√©dictions initiales ne sont pas tr√®s bonnes, mais au fur et √† mesure que le processus se poursuit, les r√©sultats pr√©dits deviennent de plus en plus pr√©cis. Si vous √™tes curieux de savoir ce qui se passe √† l'int√©rieur de la fonction `step()`, inspectez le code (bien comment√©) avec :

```py
# ??scheduler.step
```

Vous pouvez √©galement ins√©rer ce nouveau planificateur √† la place du planificateur original fourni avec le pipeline, et √©chantillonner de la mani√®re suivante :

```py
image_pipe.scheduler = scheduler
images = image_pipe(num_inference_steps=40).images
images[0]
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Tr√®s bien, nous pouvons maintenant obtenir des √©chantillons dans un d√©lai raisonnable ! Cela devrait acc√©l√©rer les choses au fur et √† mesure que nous avan√ßons dans le reste de ce *notebook* :)


## Finetuning

Et maintenant, le plus amusant ! √âtant donn√© ce pipeline pr√©-entra√Æn√©, comment pouvons-nous r√©entra√Æner le mod√®le pour g√©n√©rer des images sur la base de nouvelles donn√©es d'entra√Ænement ?

Il s'av√®re que cela est presque identique √† entra√Æner un mod√®le √† partir de z√©ro (comme nous l'avons vu dans l'unit√© 1), sauf que nous commen√ßons avec le mod√®le existant. Voyons cela en action et abordons quelques consid√©rations suppl√©mentaires au fur et √† mesure.

Tout d'abord, le jeu de donn√©es : vous pouvez essayer ce [jeu de donn√©es de visages vintage](https://huggingface.co/datasets/Norod78/Vintage-Faces-FFHQAligned) ou ces [visages anim√©s](https://huggingface.co/datasets/huggan/anime-faces) pour quelque chose de plus proche des donn√©es d'entra√Ænement originales de ce mod√®le de visages. Mais pour le plaisir, utilisons plut√¥t le m√™me petit jeu de donn√©es de papillons que nous avons utilis√© pour nous entra√Æner √† partir de z√©ro dans l'unit√© 1. Ex√©cutez le code ci-dessous pour t√©l√©charger le jeu de donn√©es papillons et cr√©er un chargeur de donn√©es √† partir duquel nous pouvons √©chantillonner un batch d'images :

```py
# Pas sur Colab ? Les commentaires avec #@ permettent de modifier l'interface utilisateur comme les titres ou les entr√©es
# mais peuvent √™tre ignor√©s si vous travaillez sur une plateforme diff√©rente.

dataset_name = "huggan/smithsonian_butterflies_subset"  # @param
dataset = load_dataset(dataset_name, split="train")
image_size = 256  # @param
batch_size = 4  # @param
preprocess = transforms.Compose(
    [
        transforms.Resize((image_size, image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ]
)


def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}


dataset.set_transform(transform)

train_dataloader = torch.utils.data.DataLoader(
    dataset, batch_size=batch_size, shuffle=True
)

print("Previewing batch:")
batch = next(iter(train_dataloader))
grid = torchvision.utils.make_grid(batch["images"], nrow=4)
plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

**Consid√©ration 1 :** notre taille de batch ici (4) est assez petite, puisque nous entra√Ænons sur une grande taille d'image (256 pixels) en utilisant un mod√®le assez grand et que nous manquerons de RAM du GPU si nous augmentons trop la taille du batch. Vous pouvez r√©duire la taille de l'image pour acc√©l√©rer les choses et permettre des batchs plus importants, mais ces mod√®les ont √©t√© con√ßus et entra√Æn√©s √† l'origine pour une g√©n√©ration de 256 pixels.

Passons maintenant √† la boucle d'entra√Ænement. Nous allons mettre √† jour les poids du mod√®le pr√©-entra√Æn√© en fixant la cible d'optimisation √† `image_pipe.unet.parameters()`. Le reste est presque identique √† l'exemple de boucle d'entra√Ænement de l'unit√© 1. Cela prend environ 10 minutes √† ex√©cuter sur Colab, c'est donc le bon moment pour prendre un caf√© ou un th√© pendant que vous attendez :

```py
num_epochs = 2  # @param
lr = 1e-5  # 2param
grad_accumulation_steps = 2  # @param

optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)

losses = []

for epoch in range(num_epochs):
    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
        clean_images = batch["images"].to(device)
        # bruit √† ajouter aux images
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]

        # un pas de temps al√©atoire pour chaque image
        timesteps = torch.randint(
            0,
            image_pipe.scheduler.num_train_timesteps,
            (bs,),
            device=clean_images.device,
        ).long()

        # Ajouter du bruit aux images propres en fonction de la magnitude du bruit √† chaque pas de temps
        # (il s'agit du processus de diffusion vers l'avant)
        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)

        # Obtenir la pr√©diction du mod√®le pour le bruit
        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]

        # Comparez la pr√©diction avec le bruit r√©el :
        loss = F.mse_loss(
            noise_pred, noise
        )  # NB : essayer de pr√©dire le bruit (eps) pas (noisy_ims-clean_ims) ou juste (clean_ims)

        # Stocker pour un plot ult√©rieur
        losses.append(loss.item())

        # Mettre √† jour les param√®tres du mod√®le avec l'optimiseur sur la base de cette perte
        loss.backward(loss)

        # Accumulation des gradients
        if (step + 1) % grad_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

    print(
        f"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}"
    )

# Tracer la courbe de perte :
plt.plot(losses)
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

**Consid√©ration 2 :** notre signal de perte est extr√™mement bruyant, puisque nous ne travaillons qu'avec quatre exemples √† des niveaux de bruit al√©atoires pour chaque √©tape. Ce n'est pas id√©al pour l'entra√Ænement. Une solution consiste √† utiliser un taux d'apprentissage extr√™mement faible pour limiter la taille de la mise √† jour √† chaque √©tape. Ce serait encore mieux si nous pouvions trouver un moyen d'obtenir les m√™mes avantages qu'en utilisant une taille de batch plus importante sans que les besoins en m√©moire ne montent en fl√®che...

Entrez dans [l'accumulation des gradients](https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html#:~:text=Simplement%20speaking%2C%20gradient%20accumulation%20means,may%20find%20 this%20tutorial%20use.). Si nous appelons `loss.backward()` plusieurs fois avant d'ex√©cuter `optimizer.step()` et `optimizer.zero_grad()`, PyTorch accumule (somme) les gradients, fusionnant effectivement le signal de plusieurs batchs pour donner une seule (meilleure) estimation qui est ensuite utilis√©e pour mettre √† jour les param√®tres. Il en r√©sulte moins de mises √† jour totales, tout comme nous le verrions si nous utilisions une taille de batch plus importante. C'est quelque chose que de nombreux *frameworks* g√®rent pour vous (par exemple, ü§ó [Accelerate rend cela facile](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)), mais il est agr√©able de le voir mis en ≈ìuvre √† partir de z√©ro car il s'agit d'une technique utile pour traiter l'entra√Ænement sous les contraintes de m√©moire du GPU ! Comme vous pouvez le voir dans le code ci-dessus (apr√®s le commentaire # Gradient accumulation), il n'y a pas vraiment besoin de beaucoup de code.

<Tip> 
‚úèÔ∏è *A votre tour !* voyez si vous pouvez ajouter l'accumulation des gradients √† la boucle d'entra√Ænement de l'unit√© 1.
Comment se comporte-t-elle ? R√©fl√©chissez √† la mani√®re dont vous pourriez ajuster le taux d'apprentissage en fonction du nombre d'√©tapes d'accumulation des gradients ; devrait-il rester identique √† auparavant ?
</Tip>

**Consid√©ration 3 :** Cela prend encore beaucoup de temps, et afficher une mise √† jour d'une ligne √† chaque √©poque n'est pas suffisant pour nous donner une bonne id√©e de ce qui se passe. Nous devrions probablement :
- G√©n√©rer quelques √©chantillons de temps en temps pour examiner visuellement la performance qualitativement au fur et √† mesure que le mod√®le s'entra√Æne.
- Enregistrer des √©l√©ments tels que la perte et les g√©n√©rations d'√©chantillons pendant l'entra√Ænement, peut-√™tre en utilisant quelque chose comme Weights and Biases ou Tensorboard.

Nous avons cr√©√© un script rapide (finetune_model.py) qui reprend le code d'entra√Ænement ci-dessus et y ajoute une fonctionnalit√© minimale de *logging*. Vous pouvez voir les [logs d'un entra√Ænement ci-dessous](https://wandb.ai/johnowhitaker/dm_finetune/runs/2upaa341) :

```py
%wandb johnowhitaker/dm_finetune/2upaa341 # Vous aurez besoin d'un compte W&B pour que cela fonctionne - sautez si vous ne voulez pas vous connecter.
```   

Il est amusant de voir comment les √©chantillons g√©n√©r√©s changent au fur et √† mesure que l'entra√Ænement progresse. M√™me si la perte ne semble pas s'am√©liorer beaucoup, on peut voir une progression du domaine original (images de chambres √† coucher) vers les nouvelles donn√©es d'entra√Ænement (wikiart). A la fin de ce *notebook* se trouve un code comment√© pour *finetun√©* un mod√®le en utilisant ce script comme alternative √† l'ex√©cution de la cellule ci-dessus.

<Tip> 
‚úèÔ∏è *A votre tour !* voyez si vous pouvez modifier l'exemple officiel de script d'entra√Ænement que nous avons vu dans l'unit√© 1 pour commencer avec un mod√®le pr√©-entra√Æn√© plut√¥t que d'entra√Æner √† partir de z√©ro.
Comparez-le au script minimal dont le lien figure ci-dessus ; quelles sont les fonctionnalit√©s suppl√©mentaires qui manquent au script minimal ?
</Tip>
    
En g√©n√©rant quelques images avec ce mod√®le, nous pouvons voir que ces visages ont d√©j√† l'air tr√®s √©tranges !

```py
x = torch.randn(8, 3, 256, 256).to(device)  # Batch de 8
for i, t in tqdm(enumerate(scheduler.timesteps)):
    model_input = scheduler.scale_model_input(x, t)
    with torch.no_grad():
        noise_pred = image_pipe.unet(model_input, t)["sample"]
    x = scheduler.step(noise_pred, t, x).prev_sample
grid = torchvision.utils.make_grid(x, nrow=4)
plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

**Consid√©ration 4 :** Le *finetuning* peut √™tre tout √† fait impr√©visible ! Si nous entra√Ænions plus longtemps, nous pourrions voir des papillons parfaits. Mais les √©tapes interm√©diaires peuvent √™tre extr√™mement int√©ressantes en elles-m√™mes, surtout si vos int√©r√™ts sont plut√¥t artistiques ! Entra√Ænez sur des p√©riodes tr√®s courtes ou tr√®s longues et faites varier le taux d'apprentissage pour voir comment cela affecte les types de r√©sultats produits par le mod√®le final.

## Code pour *finetuner* un mod√®le en utilisant le script d'exemple minimal que nous avons utilis√© sur le mod√®le de d√©monstration WikiArt

Si vous souhaitez entra√Æner un mod√®le similaire √† celui que nous avons cr√©√© sur WikiArt, vous pouvez d√©commenter et ex√©cuter les cellules ci-dessous. Comme cela prend un certain temps et peut √©puiser la m√©moire de votre GPU, nous vous conseillons de le faire apr√®s avoir parcouru le reste de ce *notebook*.

```py
## Pour t√©l√©charger le script de finetuning :
# !wget https://github.com/huggingface/diffusion-models-class/raw/main/unit2/finetune_model.py
```   

```py
## Pour ex√©cuter le script, entra√Ænant le mod√®le de visage sur des visages vintage
## (l'id√©al est d'ex√©cuter ce script dans un terminal) :
# !python finetune_model.py --image_size 128 --batch_size 8 --num_epochs 16\
#     --grad_accumulation_steps 2 --start_model "google/ddpm-celebahq-256"\
#     --dataset_name "Norod78/Vintage-Faces-FFHQAligned" --wandb_project 'dm-finetune'\
#     --log_samples_every 100 --save_model_every 1000 --model_save_name 'vintageface'
```
   
## Sauvegarde et chargement des pipelines *finetun√©s*

Maintenant que nous avons *finetun√©* le UNet dans notre mod√®le de diffusion, sauvegardons-le dans un dossier local en ex√©cutant :

```py
image_pipe.save_pretrained("my-finetuned-model")
```    

Comme nous l'avons vu dans l'unit√© 1, cela permet de sauvegarder la configuration, le mod√®le et le planificateur :

```py
!ls {"my-finetuned-model"}
```   

Ensuite, vous pouvez suivre les m√™mes √©tapes que celles d√©crites dans le *notebook* d'introduction √† ü§ó *Diffusers* de l'unit√© 1 pour pousser le mod√®le vers le Hub en vue d'une utilisation ult√©rieure :

```py
# Code pour t√©l√©charger un pipeline sauvegard√© localement vers le Hub
from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name

# Mise en place du repo et t√©l√©chargement des fichiers
model_name = "ddpm-celebahq-finetuned-butterflies-2epochs"  # @param Le nom que vous souhaitez lui donner sur le Hub
local_folder_name = "my-finetuned-model"  # @param Cr√©√© par le script ou par vous via image_pipe.save_pretrained('save_name')
description = "Describe your model here"  # @param
hub_model_id = get_full_repo_name(model_name)
create_repo(hub_model_id)
api = HfApi()
api.upload_folder(
    folder_path=f"{local_folder_name}/scheduler", path_in_repo="", repo_id=hub_model_id
)
api.upload_folder(
    folder_path=f"{local_folder_name}/unet", path_in_repo="", repo_id=hub_model_id
)
api.upload_file(
    path_or_fileobj=f"{local_folder_name}/model_index.json",
    path_in_repo="model_index.json",
    repo_id=hub_model_id,
)

# Ajouter une carte mod√®le (facultatif mais sympa !)
content = f"""
---
license: mit
tags:
- pytorch
- diffusers
- unconditional-image-generation
- diffusion-models-class
---

# Example Fine-Tuned Model for Unit 2 of the [Diffusion Models Class üß®](https://github.com/huggingface/diffusion-models-class)

{description}

## Usage

```python
from diffusers import DDPMPipeline

pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')
image = pipeline().images[0]
image
```
"""

card = ModelCard(content)
card.push_to_hub(hub_model_id)
     
```
```py
'https://huggingface.co/lewtun/ddpm-celebahq-finetuned-butterflies-2epochs/blob/main/README.md'
```

F√©licitations, vous avez maintenant *finetun√©* votre premier mod√®le de diffusion !

Pour le reste de ce notebook, nous utiliserons un [mod√®le](https://huggingface.co/johnowhitaker/sd-class-wikiart-from-bedrooms) que nous avons *finetun√©* √† partir d'un mod√®le entra√Æn√© sur [LSUN bedrooms](https://huggingface.co/google/ddpm-bedroom-256) environ une fois sur le [WikiArt dataset](https://huggingface.co/datasets/huggan/wikiart). Si vous pr√©f√©rez, vous pouvez sauter cette cellule et utiliser le pipeline faces/butterflies que nous avons *finetun√©* dans la section pr√©c√©dente ou en charger un depuis le Hub √† la place :

```py
# Chargement du pipeline pr√©-entra√Æn√©
pipeline_name = "johnowhitaker/sd-class-wikiart-from-bedrooms"
image_pipe = DDPMPipeline.from_pretrained(pipeline_name).to(device)

# √âchantillon d'images avec un planificateur DDIM sur 40 √©tapes
scheduler = DDIMScheduler.from_pretrained(pipeline_name)
scheduler.set_timesteps(num_inference_steps=40)

# Point de d√©part al√©atoire (batch de 8 images)
x = torch.randn(8, 3, 256, 256).to(device)

# Boucle d'√©chantillonnage minimale
for i, t in tqdm(enumerate(scheduler.timesteps)):
    model_input = scheduler.scale_model_input(x, t)
    with torch.no_grad():
        noise_pred = image_pipe.unet(model_input, t)["sample"]
    x = scheduler.step(noise_pred, t, x).prev_sample

# Voir les r√©sultats
grid = torchvision.utils.make_grid(x, nrow=4)
plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

**Consid√©ration 5** : Il est souvent difficile de savoir si le *finetun√©* fonctionne bien, et ce que l'on entend par "bonnes performances" peut varier selon le cas d'utilisation. Par exemple, si vous *finetun√©* un mod√®le conditionn√© par du texte comme Stable Diffusion sur un petit jeu de donn√©es, vous voudrez probablement qu'il conserve la plus grande partie de son apprentissage original afin de pouvoir comprendre des prompts arbitraires non couverts par votre nouveau jeu de donn√©es, tout en s'adaptant pour mieux correspondre au style de vos nouvelles donn√©es d'entra√Ænement. Cela pourrait signifier l'utilisation d'un faible taux d'apprentissage avec quelque chose comme la moyenne exponentielle du mod√®le, comme d√©montr√© dans cet excellent [article de blog](https://lambdalabs.com/blog/how-to-fine-tune-stable-diffusion-how-we-made-the-text-to-pokemon-model-at-lambda) sur la cr√©ation d'une version Pokemon de Stable Diffusion. Dans une autre situation, vous pouvez vouloir r√©-entra√Æner compl√®tement un mod√®le sur de nouvelles donn√©es (comme notre exemple chambre -> wikiart), auquel cas un taux d'apprentissage plus √©lev√© et un entra√Ænement plus pouss√© s'av√®rent judicieux. M√™me si le [graphique de la perte] (https://wandb.ai/johnowhitaker/dm_finetune/runs/2upaa341) ne montre pas beaucoup d'am√©lioration, les √©chantillons s'√©loignent clairement des donn√©es d'origine et s'orientent vers des r√©sultats plus "artistiques", bien qu'ils restent pour la plupart incoh√©rents.

Ce qui nous am√®ne √† la section suivante, o√π nous examinons comment nous pourrions ajouter des conseils suppl√©mentaires √† un tel mod√®le pour mieux contr√¥ler les r√©sultats.


## Guidage

Que faire si l'on souhaite exercer un certain contr√¥le sur les √©chantillons g√©n√©r√©s ? Par exemple, supposons que nous voulions biaiser les images g√©n√©r√©es pour qu'elles soient d'une couleur sp√©cifique. Comment proc√©der ? C'est l√† qu'intervient le guidage, une technique qui permet d'ajouter un contr√¥le suppl√©mentaire au processus d'√©chantillonnage.

La premi√®re √©tape consiste √† cr√©er notre fonction de conditionnement : une mesure (perte) que nous souhaitons minimiser. En voici une pour l'exemple de la couleur, qui compare les pixels d'une image √† une couleur cible (par d√©faut, une sorte de sarcelle claire) et renvoie l'erreur moyenne :

```py
def color_loss(images, target_color=(0.1, 0.9, 0.5)):
    """√âtant donn√© une couleur cible (R, G, B), retourner une perte correspondant √† la distance moyenne entre 
	les pixels de l'image et cette couleur. Par d√©faut, il s'agit d'une couleur sarcelle claire : (0.1, 0.9, 0.5)"""
    target = (
        torch.tensor(target_color).to(images.device) * 2 - 1
    )  # Map target color to (-1, 1)
    target = target[
        None, :, None, None
    ]  # Obtenir la forme n√©cessaire pour fonctionner avec les images (b, c, h, w)
    error = torch.abs(
        images - target
    ).mean()  # Diff√©rence absolue moyenne entre les pixels de l'image et la couleur cible
    return error
```     

Ensuite, nous allons cr√©er une version modifi√©e de la boucle d'√©chantillonnage o√π, √† chaque √©tape, nous ferons ce qui suit :
- Cr√©er une nouvelle version de `x` avec `requires_grad = True`
- Calculer la version d√©bruit√©e (`x0`)
- Introduire la version pr√©dite `x0` dans notre fonction de perte
- Trouver le gradient de cette fonction de perte par rapport √† `x`
- Utiliser ce gradient de conditionnement pour modifier `x` avant d'utiliser le planificateur, en esp√©rant pousser x dans une direction qui conduira √† une perte plus faible selon notre fonction d'orientation.

Il existe deux variantes que vous pouvez explorer. Dans la premi√®re, nous fixons `requires_grad` sur `x` apr√®s avoir obtenu notre pr√©diction de bruit du UNet, ce qui est plus efficace en termes de m√©moire (puisque nous n'avons pas √† retracer les gradients √† travers le mod√®le de diffusion), mais donne un gradient moins pr√©cis. Dans le second cas, nous d√©finissons d'abord `requires_grad` sur `x`, puis nous le faisons passer par l'unet et nous calculons le `x0` pr√©dit.


```py
# Variante 1 : m√©thode rapide

# L'√©chelle de guidance d√©termine l'intensit√© de l'effet
guidance_loss_scale = 40  # Envisagez de modifier cette valeur √† 5, ou √† 100

x = torch.randn(8, 3, 256, 256).to(device)

for i, t in tqdm(enumerate(scheduler.timesteps)):

    # Pr√©parer l'entr√©e du mod√®le
    model_input = scheduler.scale_model_input(x, t)

    # Pr√©dire le bruit r√©siduel
    with torch.no_grad():
        noise_pred = image_pipe.unet(model_input, t)["sample"]

    # Fixer x.requires_grad √† True
    x = x.detach().requires_grad_()

    # Obtenir la valeur pr√©dite x0
    x0 = scheduler.step(noise_pred, t, x).pred_original_sample

    # Calculer la perte
    loss = color_loss(x0) * guidance_loss_scale
    if i % 10 == 0:
        print(i, "loss:", loss.item())

    # Obtenir le gradient
    cond_grad = -torch.autograd.grad(loss, x)[0]

    # Modifier x en fonction de ce gradient
    x = x.detach() + cond_grad

    # Le planificateur
    x = scheduler.step(noise_pred, t, x).prev_sample

# Voir le r√©sultat
grid = torchvision.utils.make_grid(x, nrow=4)
im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5
Image.fromarray(np.array(im * 255).astype(np.uint8))
```
```py
0 loss: 27.279136657714844
10 loss: 11.286816596984863
20 loss: 10.683112144470215
30 loss: 10.942476272583008
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Cette deuxi√®me option n√©cessite presque le double de RAM GPU pour fonctionner, m√™me si nous ne g√©n√©rons qu'un batch de quatre images au lieu de huit. Voyez si vous pouvez rep√©rer la diff√©rence et r√©fl√©chissez √† la raison pour laquelle cette m√©thode est plus "pr√©cise" :

```py
# Variante 2 : d√©finir x.requires_grad avant de calculer les pr√©dictions du mod√®le

guidance_loss_scale = 40
x = torch.randn(4, 3, 256, 256).to(device)

for i, t in tqdm(enumerate(scheduler.timesteps)):

    # D√©finir requires_grad avant la passe avant du mod√®le
    x = x.detach().requires_grad_()
    model_input = scheduler.scale_model_input(x, t)

    # pr√©dire (avec grad cette fois)
    noise_pred = image_pipe.unet(model_input, t)["sample"]

    # Obtenir la valeur pr√©dite x0 :
    x0 = scheduler.step(noise_pred, t, x).pred_original_sample

    # Calculer la perte
    loss = color_loss(x0) * guidance_loss_scale
    if i % 10 == 0:
        print(i, "loss:", loss.item())

    # Obtenir le gradient
    cond_grad = -torch.autograd.grad(loss, x)[0]

    # Modifier x en fonction de ce gradient
    x = x.detach() + cond_grad

    # Le planificateur
    x = scheduler.step(noise_pred, t, x).prev_sample


grid = torchvision.utils.make_grid(x, nrow=4)
im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5
Image.fromarray(np.array(im * 255).astype(np.uint8))
```   
```py
0 loss: 30.750328063964844
10 loss: 18.550724029541016
20 loss: 17.515094757080078
30 loss: 17.55681037902832
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Dans la seconde variante, les besoins en m√©moire sont plus importants et l'effet est moins prononc√©, de sorte que vous pouvez penser qu'elle est inf√©rieure. Cependant, les r√©sultats sont sans doute plus proches des types d'images sur lesquels le mod√®le a √©t√© entra√Æn√©, et vous pouvez toujours augmenter l'√©chelle de guidage pour obtenir un effet plus important. L'approche que vous utiliserez d√©pendra en fin de compte de ce qui fonctionne le mieux sur le plan exp√©rimental.

<Tip> 
‚úèÔ∏è *A votre tour !* choisissez votre couleur pr√©f√©r√©e et recherchez ses valeurs dans l'espace RGB.
Modifiez la ligne `color_loss()` dans la cellule ci-dessus pour recevoir ces nouvelles valeurs RGB et examinez les r√©sultats ; correspondent-ils √† ce que vous attendez ?
</Tip>

## Guidage avec CLIP

Guider vers une couleur nous donne un peu de contr√¥le, mais que se passerait-il si nous pouvions simplement taper un texte d√©crivant ce que nous voulons ?

[CLIP](https://openai.com/research/clip) est un mod√®le cr√©√© par OpenAI qui nous permet de comparer des images √† des l√©gendes textuelles. C'est extr√™mement puissant, car cela nous permet de quantifier √† quel point une image correspond √† un prompt. Et comme le processus est diff√©rentiable, nous pouvons l'utiliser comme fonction de perte pour guider notre mod√®le de diffusion !

Nous n'entrerons pas dans les d√©tails ici. L'approche de base est la suivante :
- Ench√¢sser le prompt pour obtenir un ench√¢ssement CLIP √† 512 dimensions
- Pour chaque √©tape du processus du mod√®le de diffusion :
	- Cr√©er plusieurs variantes de l'image d√©bruit√©e pr√©dite (le fait d'avoir plusieurs variantes permet d'obtenir un signal de perte plus propre).
	- Pour chacune d'entre elles, ench√¢sser l'image avec CLIP et comparez cet ench√¢ssement avec celui du prompt (√† l'aide d'une mesure appel√©e "distance du grand cercle").
- Calculer le gradient de cette perte par rapport √† l'image bruyante actuelle x et utiliser ce gradient pour modifier x avant de le mettre √† jour avec le planificateur.

Pour une explication plus approfondie de CLIP, consultez cette [le√ßon sur le sujet] (https://johnowhitaker.github.io/tglcourse/clip.html) ou ce [rapport sur le projet OpenCLIP] (https://wandb.ai/johnowhitaker/openclip-benchmarking/reports/Exploring-OpenCLIP--VmlldzoyOTIzNzIz) que nous utilisons pour charger le mod√®le CLIP. Ex√©cutez la cellule suivante pour charger un mod√®le CLIP :

```py
import open_clip

clip_model, _, preprocess = open_clip.create_model_and_transforms(
    "ViT-B-32", pretrained="openai"
)
clip_model.to(device)

# Transformations pour redimensionner et augmenter une image + normalisation pour correspondre aux donn√©es entra√Æn√©es par CLIP
tfms = torchvision.transforms.Compose(
    [
        torchvision.transforms.RandomResizedCrop(224),  # CROP al√©atoire √† chaque fois
        torchvision.transforms.RandomAffine(
            5
        ),  # Une augmentation al√©atoire possible : biaiser l'image
        torchvision.transforms.RandomHorizontalFlip(),  # Vous pouvez ajouter des augmentations suppl√©mentaires si vous le souhaitez
        torchvision.transforms.Normalize(
            mean=(0.48145466, 0.4578275, 0.40821073),
            std=(0.26862954, 0.26130258, 0.27577711),
        ),
    ]
)

# Et d√©finir une fonction de perte qui prend une image, l'ench√¢sse et la compare avec les caract√©ristiques textuelles du prompt
def clip_loss(image, text_features):
    image_features = clip_model.encode_image(
        tfms(image)
    )  # Note : applique les transformations ci-dessus
    input_normed = torch.nn.functional.normalize(image_features.unsqueeze(1), dim=2)
    embed_normed = torch.nn.functional.normalize(text_features.unsqueeze(0), dim=2)
    dists = (
        input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)
    )  # Distance du grand cercle
    return dists.mean()
```

Une fois la fonction de perte d√©finie, notre boucle d'√©chantillonnage guid√© ressemble aux exemples pr√©c√©dents, en rempla√ßant `color_loss()` par notre nouvelle fonction de perte bas√©e sur CLIP :
```py
prompt = "Red Rose (still life), red flower painting"  # @param

# Explorer en changeant √ßa
guidance_scale = 8  # @param
n_cuts = 4  # @param

# Plus d'√©tapes -> plus de temps pour que le guidage ait un effet
scheduler.set_timesteps(50)

# Nous ench√¢ssons un prompt avec CLIP comme cible
text = open_clip.tokenize([prompt]).to(device)
with torch.no_grad(), torch.cuda.amp.autocast():
    text_features = clip_model.encode_text(text)


x = torch.randn(4, 3, 256, 256).to(
    device
)  # L'utilisation de la RAM est √©lev√©e, vous ne voulez peut-√™tre qu'une seule image √† la fois.

for i, t in tqdm(enumerate(scheduler.timesteps)):

    model_input = scheduler.scale_model_input(x, t)

    # pr√©dire le bruit r√©siduel
    with torch.no_grad():
        noise_pred = image_pipe.unet(model_input, t)["sample"]

    cond_grad = 0

    for cut in range(n_cuts):

        # n√©cessite un grad sur x
        x = x.detach().requires_grad_()

        # Obtenir le x0 pr√©dit
        x0 = scheduler.step(noise_pred, t, x).pred_original_sample

        # Calculer la perte
        loss = clip_loss(x0, text_features) * guidance_scale

        # Obtenir le gradient (√©chelle par n_cuts puisque nous voulons la moyenne)
        cond_grad -= torch.autograd.grad(loss, x)[0] / n_cuts

    if i % 25 == 0:
        print("Step:", i, ", Guidance loss:", loss.item())

    # Modifier x en fonction de ce gradient
    alpha_bar = scheduler.alphas_cumprod[i]
    x = (
        x.detach() + cond_grad * alpha_bar.sqrt()
    )  # Note the additional scaling factor here!

    # Le planificateur
    x = scheduler.step(noise_pred, t, x).prev_sample


grid = torchvision.utils.make_grid(x.detach(), nrow=4)
im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5
Image.fromarray(np.array(im * 255).astype(np.uint8))
```
```py
Step: 0 , Guidance loss: 7.437869548797607
Step: 25 , Guidance loss: 7.174620628356934
```

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Cela ressemble un peu √† des roses ! Ce n'est pas parfait, mais si vous jouez avec les param√®tres, vous pouvez obtenir des images agr√©ables.

Si vous examinez le code ci-dessus, vous verrez que nous mettons √† l'√©chelle le gradient de conditionnement par un facteur de `alpha_bar.sqrt()`. Il existe des th√©ories sur la "bonne" mani√®re d'√©chelonner ces gradients, mais en pratique, vous pouvez exp√©rimenter. Pour certains types de guidage, vous voudrez peut-√™tre que la plupart des effets soient concentr√©s dans les premi√®res √©tapes, pour d'autres (par exemple, une perte de style ax√©e sur les textures), vous pr√©f√©rerez peut-√™tre qu'ils n'interviennent que vers la fin du processus de g√©n√©ration. Quelques programmes possibles sont pr√©sent√©s ci-dessous :

```py
plt.plot([1 for a in scheduler.alphas_cumprod], label="no scaling")
plt.plot([a for a in scheduler.alphas_cumprod], label="alpha_bar")
plt.plot([a.sqrt() for a in scheduler.alphas_cumprod], label="alpha_bar.sqrt()")
plt.plot(
    [(1 - a).sqrt() for a in scheduler.alphas_cumprod], label="(1-alpha_bar).sqrt()"
)
plt.legend()
plt.title("Possible guidance scaling schedules")
```  

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Exp√©rimentez avec diff√©rents planificateurs, √©chelles de guidage et toute autre astuce √† laquelle vous pouvez penser (l'√©cr√™tage des gradients dans une certaine plage est une modification populaire) pour voir jusqu'√† quel point vous pouvez obtenir ce r√©sultat ! N'oubliez pas non plus d'essayer d'intervertir d'autres mod√®les. Peut-√™tre le mod√®le de visages que nous avons charg√© au d√©but ; pouvez-vous le guider de mani√®re fiable pour produire un visage masculin ? Que se passe-t-il si vous combinez le guidage CLIP avec la perte de couleur que nous avons utilis√©e plus t√¥t ? Etc.

Si vous consultez [quelques codes pour la diffusion guid√©e par CLIP en pratique](https://huggingface.co/spaces/EleutherAI/clip-guided-diffusion/blob/main/app.py), vous verrez une approche plus complexe avec une meilleure classe pour choisir des d√©coupes al√©atoires dans les images et de nombreux ajustements suppl√©mentaires de la fonction de perte pour de meilleures performances. Avant l'apparition des mod√®les de diffusion conditionn√©s par le texte, il s'agissait du meilleur syst√®me de conversion texte-image qui soit ! La petite version de notre jouet peut encore √™tre am√©lior√©e, mais elle capture l'id√©e principale : gr√¢ce au guidage et aux capacit√©s √©tonnantes de CLIP, nous pouvons ajouter le contr√¥le du texte √† un mod√®le de diffusion inconditionnel üé®.


## Partager une boucle d'√©chantillonnage personnalis√©e en tant que d√©mo Gradio

Vous avez peut-√™tre trouv√© une perte amusante pour guider la g√©n√©ration et vous souhaitez maintenant partager avec le monde entier votre mod√®le *finetun√©* et cette strat√©gie d'√©chantillonnage personnalis√©e...

Entrez dans [Gradio](https://gradio.app/). Gradio est un outil gratuit et open-source qui permet aux utilisateurs de cr√©er et de partager facilement des mod√®les interactifs d'apprentissage automatique via une simple interface web. Avec Gradio, les utilisateurs peuvent construire des interfaces personnalis√©es pour leurs mod√®les d'apprentissage automatique, qui peuvent ensuite √™tre partag√©s avec d'autres par le biais d'une URL unique. Il est √©galement int√©gr√© √† ü§ó Spaces, ce qui permet d'h√©berger facilement des d√©mos et de les partager avec d'autres.

Nous placerons notre logique de base dans une fonction qui prend certaines entr√©es et produit une image en sortie. Cette fonction peut ensuite √™tre envelopp√©e dans une interface simple qui permet √† l'utilisateur de sp√©cifier certains param√®tres (qui sont transmis en tant qu'entr√©es √† la fonction principale de g√©n√©ration). De nombreux [composants](https://gradio.app/docs/#components) sont disponibles ; pour cet exemple, nous utiliserons un curseur pour l'√©chelle d'orientation et un s√©lecteur de couleurs pour d√©finir la couleur cible.

```py
!pip install -q gradio
```  
 
```py  
import gradio as gr
from PIL import Image, ImageColor

# La fonction qui fait le gros du travail
def generate(color, guidance_loss_scale):
    target_color = ImageColor.getcolor(color, "RGB")  # Couleur cible en RGB
    target_color = [a / 255 for a in target_color]  # R√©√©chelonner de (0, 255) √† (0, 1)
    x = torch.randn(1, 3, 256, 256).to(device)
    for i, t in tqdm(enumerate(scheduler.timesteps)):
        model_input = scheduler.scale_model_input(x, t)
        with torch.no_grad():
            noise_pred = image_pipe.unet(model_input, t)["sample"]
        x = x.detach().requires_grad_()
        x0 = scheduler.step(noise_pred, t, x).pred_original_sample
        loss = color_loss(x0, target_color) * guidance_loss_scale
        cond_grad = -torch.autograd.grad(loss, x)[0]
        x = x.detach() + cond_grad
        x = scheduler.step(noise_pred, t, x).prev_sample
    grid = torchvision.utils.make_grid(x, nrow=4)
    im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5
    im = Image.fromarray(np.array(im * 255).astype(np.uint8))
    im.save("test.jpeg")
    return im


# Voir la documentation de gradio pour les types d'entr√©es et de sorties disponibles.
inputs = [
    gr.ColorPicker(label="color", value="55FFAA"),  # Ajoutez ici toutes les entr√©es dont vous avez besoin
    gr.Slider(label="guidance_scale", minimum=0, maximum=30, value=3),
]
outputs = gr.Image(label="result")

# Et l'interface minimale
demo = gr.Interface(
    fn=generate,
    inputs=inputs,
    outputs=outputs,
    examples=[
        ["#BB2266", 3],
        ["#44CCAA", 5],  # Vous pouvez fournir des exemples d'entr√©es pour aider les gens √† d√©marrer
    ],
)
demo.launch(debug=True)  # debug=True vous permet de voir les erreurs et les sorties dans Colab
```  
 
Il est possible de construire des interfaces beaucoup plus compliqu√©es, avec un style fantaisiste et un large √©ventail d'entr√©es possibles, mais pour cette d√©mo, nous la gardons aussi simple que possible.

Les d√©mos sur ü§ó Spaces s'ex√©cutent par d√©faut sur CPU, il est donc pr√©f√©rable de prototyper votre interface dans Colab (comme ci-dessus) avant de la migrer. Lorsque vous √™tes pr√™t √† partager votre d√©mo, vous devez cr√©er un Space, mettre en place un fichier `requirements.txt` listant les biblioth√®ques que votre code utilisera, puis placer tout le code dans un fichier `app.py` qui d√©finit les fonctions pertinentes et l'interface.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aper√ßu du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aper√ßu des diff√©rents chapitres du cours.">
</div>

Heureusement pour vous, il est √©galement possible de "dupliquer" un Space. Vous pouvez visiter le Space [ici](https://huggingface.co/spaces/johnowhitaker/color-guided-wikiart-diffusion) et cliquer sur "Dupliquer cet espace" pour obtenir un mod√®le que vous pouvez ensuite modifier pour utiliser votre propre mod√®le et votre propre fonction d'orientation.

Dans les param√®tres, vous pouvez configurer votre Space pour qu'il fonctionne avec du mat√©riel plus sophistiqu√© (qui est factur√© √† l'heure). Vous avez cr√©√© quelque chose d'extraordinaire et vous voulez le partager sur un meilleur mat√©riel, mais vous n'avez pas l'argent n√©cessaire ? Faites-le nous savoir via Discord et nous verrons si nous pouvons vous aider !

## R√©sum√© et prochaines √©tapes

Nous avons couvert beaucoup de choses dans ce *notebook* ! R√©capitulons les id√©es principales :
- Il est relativement facile de charger des mod√®les existants et de les √©chantillonner avec diff√©rents planificateurs
- Le *finetuning* ressemble √† l'entra√Ænement √† partir de z√©ro, sauf qu'en partant d'un mod√®le existant, nous esp√©rons obtenir de meilleurs r√©sultats plus rapidement.
- Pour *finetuner* de grands mod√®les sur de grandes images, nous pouvons utiliser des astuces comme l'accumulation de gradient pour contourner les limitations de la taille des batchs.
- L'enregistrement d'√©chantillons d'images est important pour le *finetuning*, o√π une courbe de perte peut ne pas fournir beaucoup d'informations utiles.
- Le guidage nous permet de prendre un mod√®le inconditionnel et d'orienter le processus de g√©n√©ration sur la base d'une fonction de guidage/perte, o√π √† chaque √©tape nous trouvons le gradient de la perte par rapport √† l'image bruit√©e $x$ et l'actualisons en fonction de ce gradient avant de passer √† l'√©tape temporelle suivante.
- Le guidage avec CLIP nous permet de contr√¥ler des mod√®les inconditionnels avec du texte !

Pour mettre cela en pratique, voici quelques √©tapes sp√©cifiques que vous pouvez suivre :
- *Finetun√©* votre propre mod√®le et le pousser vers le Hub. Cela implique de choisir un point de d√©part (par exemple, un mod√®le entra√Æn√© sur [faces](https://huggingface.co/google/ddpm-celebahq-256), [bedrooms](https://huggingface.co/fusing/ddpm-lsun-bedroom), [cats](https://huggingface.co/fusing/ddpm-lsun-cat) ou [wikiart](https://huggingface.co/johnowhitaker/sd-class-wikiart-from-bedrooms) et un jeu de donn√©es (peut-√™tre ces [faces d'animaux](https://huggingface.co/datasets/huggan/AFHQv2) ou vos propres images), puis d'entra√Æner soit le code de ce *notebook*, soit le script d'exemple (utilisation de d√©monstration ci-dessous).
- Explorer le guidage en utilisant votre mod√®le *finetun√©*, soit en utilisant l'une des fonctions de guidage de l'exemple (color_loss ou CLIP), soit en inventant la v√¥tre.
- Partagez une d√©mo bas√©e sur ceci en utilisant Gradio, soit en modifiant le [Space d'exemple](https://huggingface.co/spaces/johnowhitaker/color-guided-wikiart-diffusion) pour utiliser votre propre mod√®le, soit en cr√©ant votre propre version personnalis√©e avec plus de fonctionnalit√©s.

Nous sommes impatients de voir vos r√©sultats sur Discord, Twitter et ailleurs ü§ó !