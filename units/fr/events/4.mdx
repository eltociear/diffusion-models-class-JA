# Sprint ControlNet en JAX/Diffusers

Bienvenue au sprint communautaire en JAX/Diffusers ! L'objectif de ce sprint est de travailler sur des mod√®les de diffusion amusants et cr√©atifs en utilisant JAX et Diffusers.

Lors de cet √©v√©nement, nous cr√©erons diverses applications avec des mod√®les de diffusion en JAX/Flax et Diffusers en utilisant des heures TPU gratuites g√©n√©reusement fournies par Google Cloud.

Ce document pr√©sente toutes les informations importantes pour faire une soumission au sprint.
 
## Organisation 

Les participants peuvent proposer des id√©es pour un projet int√©ressant impliquant des mod√®les de diffusion. Des √©quipes de 3 √† 5 personnes seront ensuite form√©es autour des projets les plus prometteurs et les plus int√©ressants. Assurez-vous de lire la section Communication pour savoir comment proposer des projets, commenter les id√©es de projet des autres participants et cr√©er une √©quipe.

Pour aider chaque √©quipe √† mener √† bien son projet, nous organiserons des conf√©rences donn√©es par des scientifiques et des ing√©nieurs de Google, de Hugging Face et de la communaut√© open source. Les conf√©rences auront lieu le 17 avril. Assurez-vous d'assister aux conf√©rences pour tirer le meilleur parti de votre participation ! Consultez la section Conf√©rences pour avoir une vue d'ensemble des conf√©rences, y compris l'orateur et l'heure de la conf√©rence.

Chaque √©quipe b√©n√©ficiera ensuite d'un **acc√®s gratuit √† une VM TPU v4-8** du 14 avril au 1er mai. De plus, nous fournirons un exemple d'entra√Ænement en JAX/Flax et Diffusers pour entra√Æner un [ControlNet](https://huggingface.co/blog/controlnet) afin de lancer votre projet. Nous fournirons √©galement des exemples sur la fa√ßon de pr√©parer les jeux de donn√©es. Pendant le sprint, nous nous assurerons de r√©pondre √† toutes les questions que vous pourriez avoir sur JAX/Flax et Diffusers et nous aiderons chaque √©quipe autant que possible !

> Nous ne distribuerons pas de TPU pour les √©quipes compos√©es d'un seul membre. Nous vous encourageons donc √† rejoindre une √©quipe ou √† trouver des co√©quipiers pour votre id√©e. 

√Ä la fin du sprint, chaque soumission sera √©valu√©e par un jury et les trois meilleures d√©monstrations recevront un prix. Consultez la section Comment soumettre une d√©mo pour plus d'informations et de suggestions sur la mani√®re de soumettre votre projet.

> üí° Note : M√™me si nous fournissons un exemple pour entra√Æner ControlNet, les participants peuvent proposer des id√©es qui n'impliquent pas du tout un ControlNet du moment qu'elles sont centr√©es sur les mod√®les de diffusion.

## Dates importantes

- **29/03** Annonce officielle de la semaine de la communaut√©.
- **31/03** Commencez √† former des groupes dans le canal #jax-diffusers-ideas sur Discord. 
- **10/04** Collecte des donn√©es.
- **13/04 & 14/04 & 17/04** [Conf√©rences de lancement sur YouTube](https://www.youtube.com/watch?v=SOj2sxgvFe0). 
- **14/04 √† 17/04.** D√©but de l'acc√®s aux TPU. 
- **01/05** Fermeture de l'acc√®s aux TPU. 
- **08/05** : Annonce des 10 meilleurs projets et des prix.

> üí° Note : Nous accepterons les candidatures tout au long du sprint.


## Communication

Toutes les communications importantes auront lieu sur notre serveur Discord. Rejoignez le serveur en utilisant [ce lien] (https://hf.co/join/discord). Apr√®s avoir rejoint le serveur, prenez le r√¥le Diffusers dans le canal `#role-assignment` et dirigez-vous vers le canal `#jax-diffusers-ideas` pour partager votre id√©e sous la forme d'un message de forum. Pour vous inscrire, remplissez le formulaire d'inscription et nous vous donnerons acc√®s √† deux canaux Discord suppl√©mentaires pour les discussions et le support technique, ainsi qu'un acc√®s aux TPU.
Les annonces importantes de l'√©quipe Hugging Face, Flax/JAX et Google Cloud seront publi√©es sur le serveur.

Le serveur Discord sera le lieu central o√π les participants pourront publier leurs r√©sultats, partager leurs exp√©riences d'apprentissage, poser des questions et obtenir une assistance technique pour les divers obstacles qu'ils rencontrent.

Pour les probl√®mes li√©s √† Flax/JAX, Diffusers, Datasets ou pour des questions sp√©cifiques √† votre projet, nous interagirons √† travers les d√©p√¥ts publics et les forums :

- Flax: [Issues](https://github.com/google/flax/issues), [Questions](https://github.com/google/flax/discussions)
- JAX: [Issues](https://github.com/google/jax/issues), [Questions](https://github.com/google/jax/discussions)
- ü§ó Diffusers: [Issues](https://github.com/huggingface/diffusers/issues), [Questions](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)
- ü§ó Datasets: [Issues](https://github.com/huggingface/datasets/issues), [Questions](https://discuss.huggingface.co/c/datasets/10)
- Questions sp√©cifiques aux projets : Elles peuvent √™tre pos√©es sur le canal #jax-diffusers-ideas sur Discord.
- Questions relatives au TPU : Canal `#jax-diffusers-tpu-support` sur Discord. 
- Discussion g√©n√©rale : `#jax-diffusers-sprint channel` sur Discord.
Vous aurez acc√®s aux canaux `#jax-diffusers-tpu-support` et `#jax-diffusers-sprint` une fois que vous aurez √©t√© accept√© pour participer au sprint.

Lorsque vous demandez de l'aide, nous vous encourageons √† poster le lien vers le [forum](https://discuss.huggingface.co) sur le serveur Discord, plut√¥t que de poster directement des *issues* ou des questions. 
De cette fa√ßon, nous nous assurons que tout le monde peut b√©n√©ficier de vos questions, m√™me apr√®s la fin du sprint.

> Note : Apr√®s le 10 avril, si vous vous √™tes inscrit sur le formulaire Google, mais que vous n'√™tes pas dans le canal Discord, veuillez laisser un message sur [l'annonce officielle du forum](https://discuss.huggingface.co/t/controlling-stable-diffusion-with-jax-and-diffusers-using-v4-tpus/35187/2) et envoyer un ping √† `@mervenoyan`, `@sayakpaul`, et `@patrickvonplaten`. Il se peut que nous prenions un jour pour traiter ces demandes.

## Conf√©rences

Nous avons invit√© d'√©minents chercheurs et ing√©nieurs de Google, Hugging Face, et de la communaut√© open-source qui travaillent dans le domaine de l'IA g√©n√©rative. Nous mettrons √† jour cette section avec des liens vers les conf√©rences, alors gardez un ≈ìil ici ou sur Discord dans le canal diffusion models core-announcements et programmez vos rappels !

### **13 avril 2023**

| Intervenant	| Sujet	| Horaire	| Video |
|---|---|---|---|
[Emiel Hoogeboom, Google Brain](https://twitter.com/emiel_hoogeboom?lang=en)	| Pixel-Space Diffusion models for High Resolution Images | 4.00pm-4.40pm CEST / 7.00am-7.40am PST| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=iw2WCAGxdQ4) |
| [Apolin√°rio Passos, Hugging Face](https://twitter.com/multimodalart?lang=en)	| Introduction to Diffusers library	|  4.40pm-5.20pm CEST / 7.40am-08.20am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=iw2WCAGxdQ4)
| [Ting Chen, Google Brain](https://twitter.com/tingchenai?lang=en)	| Diffusion++: discrete data and high-dimensional generation |	 5.45pm-6.25pm CEST / 08.45am-09.25am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=iw2WCAGxdQ4) |

### **14 avril 2023**

| Intervenant	| Sujet	| Horaire	| Video |
|---|---|---|---|
| [Tim Salimans, Google Brain](https://twitter.com/timsalimans?lang=en)	| Efficient image and video generation with distilled diffusion models |   4.00pm-4.40pm CEST / 7.00am-7.40am PST| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=6f5chgbKjSg&ab_channel=HuggingFace) |
| [Suraj Patil, Hugging Face](https://twitter.com/psuraj28?lang=en)	| Masked Generative Models: MaskGIT/Muse	|  4.40pm-5.20pm CEST / 7.40am-08.20am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=6f5chgbKjSg&ab_channel=HuggingFace) |
| [Sabrina Mielke, John Hopkins University](https://twitter.com/sjmielke?lang=en)	| From stateful code to purified JAX: how to build your neural net framework |	 5.20pm-6.00pm CEST / 08.20am-09.00am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=6f5chgbKjSg&ab_channel=HuggingFace) |

### **17 avril 2023**

| Intervenant	| Sujet	| Horaire	| Video |
|---|---|---|---|
| [Andreas Steiner, Google Brain](https://twitter.com/AndreasPSteiner)	| JAX & ControlNet |  4.00pm-4.40pm CEST / 7.00am-7.40am PST| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=SOj2sxgvFe0) |
| [Boris Dayma, craiyon](https://twitter.com/borisdayma?lang=en)	| DALL-E Mini	|  4.40pm-5.20pm CEST / 7.40am-08.20am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=SOj2sxgvFe0) |
| [Margaret Mitchell, Hugging Face](https://twitter.com/mmitchell_ai?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)	| Ethics of Text-to-Image |	 5.20pm-6.00pm CEST / 08.20am-09.00am PST	| [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=SOj2sxgvFe0) |


## Donn√©es et pr√©traitement

Dans cette section, nous verrons comment construire votre propre jeu de donn√©es pour entra√Æner ControlNet.

### Pr√©parer un grand jeu de donn√©es local

#### Monter un disque 

Si vous avez besoin d'espace suppl√©mentaire, vous pouvez suivre [ce guide](https://cloud.google.com/tpu/docs/setup-persistent-disk#prerequisites) pour cr√©er un disque persistant, l'attacher √† votre VM TPU et cr√©er un r√©pertoire pour monter le disque. Vous pouvez ensuite utiliser ce r√©pertoire pour stocker votre jeu de donn√©es.

Par ailleurs, la VM TPU attribu√©e √† votre √©quipe dispose d'un disque de stockage persistant de 3 To. Pour apprendre √† l'utiliser, consultez [ce guide](https://cloud.google.com/tpu/docs/setup-persistent-disk#mount-pd). 

#### Pr√©traitement des donn√©es 

Nous montrons ici comment pr√©parer un grand jeu de donn√©es pour entra√Æner un mod√®le ControlNet avec filtre de Canny. Plus pr√©cis√©ment, nous fournissons un [exemple de script](./dataset_tools/coyo_1m_dataset_preprocess.py) qui :
* S√©lectionne 1 million de paires image-texte √† partir d'un jeu de donn√©es existant [COYO-700M](https://huggingface.co/datasets/kakaobrain/coyo-700m).
* T√©l√©charge chaque image et utilise le filtre de Canny pour g√©n√©rer l'image de conditionnement. 
* Cr√©e un m√©tafichier qui relie toutes les images et les images trait√©es √† leurs l√©gendes. 

Utilisez la commande suivante pour ex√©cuter le script de pr√©traitement des donn√©es de l'exemple. Si vous avez mont√© un disque sur votre TPU, vous devez placer vos fichiers `train_data_dir` et `cache_dir` sur le disque mont√©.

```py
python3 coyo_1m_dataset_preprocess.py \
 --train_data_dir="/mnt/disks/persist/data" \
 --cache_dir="/mnt/disks/persist" \
 --max_train_samples=1000000 \
 --num_proc=16
```

Une fois le script ex√©cut√©, vous trouverez un dossier de donn√©es dans le r√©pertoire `train_data_dir` sp√©cifi√© avec la structure de dossier ci-dessous :

```py
data
‚îú‚îÄ‚îÄ images
‚îÇ   ‚îú‚îÄ‚îÄ image_1.png
‚îÇ   ‚îú‚îÄ‚îÄ .......
‚îÇ   ‚îî‚îÄ‚îÄ image_1000000.jpeg
‚îú‚îÄ‚îÄ processed_images
‚îÇ   ‚îú‚îÄ‚îÄ image_1.png
‚îÇ   ‚îú‚îÄ‚îÄ .......
‚îÇ   ‚îî‚îÄ‚îÄ image_1000000.jpeg
‚îî‚îÄ‚îÄ meta.jsonl
```

#### Charger un jeu de donn√©es

Pour charger un jeu de donn√©es √† partir du dossier de donn√©es que vous venez de cr√©er, vous devez ajouter un script de chargement de jeu de donn√©es √† votre dossier de donn√©es. Le script de chargement de donn√©es doit porter le m√™me nom que le dossier. Par exemple, si votre dossier de donn√©es est `data`, vous devez ajouter un script de chargement de donn√©es nomm√© `data.py`. Nous fournissons un [exemple de script de chargement de donn√©es](./dataset_tools/data.py) que vous pouvez utiliser. Tout ce que vous avez √† faire est de mettre √† jour le `DATA_DIR` avec le chemin correct de votre dossier de donn√©es. Pour plus de d√©tails sur l'√©criture d'un script de chargement de donn√©es, reportez-vous √† la [documentation] (https://huggingface.co/docs/datasets/dataset_script).

Une fois que le script de chargement de donn√©es est ajout√© √† votre dossier de donn√©es, vous pouvez le charger avec : 

```py
dataset = load_dataset("/mnt/disks/persist/data", cache_dir="/mnt/disks/persist" )
```

Notez que vous pouvez utiliser `--train_data_dir` pour passer le r√©pertoire de votre dossier de donn√©es au script d'entra√Ænement et g√©n√©rer votre jeu de donn√©es automatiquement pendant l'entra√Ænement.
 
Pour les grands jeux de donn√©es, nous recommandons de g√©n√©rer le jeu de donn√©es une seule fois et de le sauvegarder sur le disque √† l'aide de la commande

```py
dataset.save_to_disk("/mnt/disks/persist/dataset")
```

Vous pouvez ensuite r√©utiliser le jeu de donn√©es sauvegard√© pour votre entra√Ænement en passant `--load_from_disk`.

Voici un exemple d'ex√©cution d'un script d'entra√Ænement qui chargera le jeu de donn√©es depuis le disque.

```py
export MODEL_DIR="runwayml/stable-diffusion-v1-5"
export OUTPUT_DIR="/mnt/disks/persist/canny_model"
export DATASET_DIR="/mnt/disks/persist/dataset"
export DISK_DIR="/mnt/disks/persist"

python3 train_controlnet_flax.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --train_data_dir=$DATASET_DIR \
 --load_from_disk \
 --cache_dir=$DISK_DIR \
 --resolution=512 \
 --learning_rate=1e-5 \
 --train_batch_size=2 \
 --revision="non-ema" \
 --from_pt \
 --max_train_steps=500000 \
 --checkpointing_steps=10000 \
 --dataloader_num_workers=16 
 ```

### Pr√©parer un jeu de donn√©es avec MediaPipe et Hugging Face 

Nous fournissons un *notebook* ([ ![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/community-events/blob/main/jax-controlnet-sprint/dataset_tools/create_pose_dataset.ipynb)) qui vous montre comment pr√©parer un jeu de donn√©es pour entra√Æner ControlNet en utilisant [MediaPipe](https://developers.google.com/mediapipe) et Hugging Face. Plus pr√©cis√©ment, dans le *notebook*, nous montrons :

* Comment tirer parti des solutions MediaPipe pour extraire les articulations du corps de la pose √† partir des images d'entr√©e.
* Pr√©dire les l√©gendes en utilisant BLIP-2 √† partir des images d'entr√©e en utilisant ü§ó Transformers.
* Construire et pousser le jeu de donn√©es final vers le Hugging Face Hub en utilisant ü§ó Datasets. 

Vous pouvez vous r√©f√©rer au *notebook* pour cr√©er vos propres jeux de donn√©es en utilisant d'autres solutions MediaPipe. Ci-dessous, nous listons toutes les solutions pertinentes :

* [D√©tection des points de rep√®re de pose](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)
* [D√©tection de rep√®res de visage](https://developers.google.com/mediapipe/solutions/vision/face_landmarker)
* [Segmentation de selfie](https://developers.google.com/mediapipe/solutions/vision/image_segmenter)


## Entra√Æner ControlNet

C'est peut-√™tre la partie la plus amusante et la plus int√©ressante de ce document, car nous vous montrons ici comment entra√Æner un mod√®le ControlNet personnalis√©. 

> üí° Note : Pour ce sprint, vous n'√™tes PAS limit√© √† entra√Æner des ControlNets. Nous fournissons ce script d'entra√Ænement comme r√©f√©rence pour vous permettre de d√©marrer. 

Pour un entra√Ænement plus rapide sur les TPU et les GPU, vous pouvez tirer parti de l'exemple d'entra√Ænement Flax. Suivez les instructions ci-dessus pour obtenir le mod√®le et le jeu de donn√©es avant d'ex√©cuter le script.

### Mise en place de la VM TPU

Avant de continuer avec le reste de cette section, vous devez vous assurer que l'adresse email que vous utilisez a √©t√© ajout√©e au projet `hf-flax` sur Google Cloud Platform. Si ce n'est pas le cas, merci de nous le faire savoir sur le serveur Discord (vous pouvez taguer `@sayakpaul`, `@merve`, et `@patrickvonplaten`).

Dans ce qui suit, nous allons d√©crire comment le faire en utilisant une console standard, mais vous devriez √©galement √™tre en mesure de vous connecter √† la VM TPU via des IDE, comme Visual Studio Code, etc.

1. Vous devez installer le [Google Cloud SDK](https://cloud.google.com/sdk/docs/install). Veuillez suivre les instructions sur https://cloud.google.com/sdk.

2. Une fois le Google Cloud SDK install√©, vous devez configurer votre compte en ex√©cutant la commande suivante. Assurez-vous que <votre-adresse-email> correspond √† l'adresse gmail que vous avez utilis√©e pour vous inscrire √† cet √©v√©nement.
  
    ```bash
    gcloud config set account <your-email-adress>
    ```

3. Assurons-nous √©galement que le bon projet est d√©fini au cas o√π votre email serait utilis√© pour plusieurs projets gcloud :

    ```bash
    gcloud config set project hf-flax
    ```

4. Ensuite, vous devez vous authentifier. Vous pouvez le faire en ex√©cutant la commande

    ```bash
    gcloud auth login
    ```

    Vous devriez obtenir un lien vers un site web o√π vous pouvez authentifier votre compte gmail.

5. Enfin, vous pouvez √©tablir un tunnel SSH dans la VM TPU ! Veuillez ex√©cuter la commande suivante en r√©glant la "--zone" sur `us-central2-b` et sur le nom de la TPU qui vous a √©t√© envoy√© par email par l'√©quipe de Hugging Face.

    ```bash
    gcloud alpha compute tpus tpu-vm ssh <tpu-name> --zone <zone> --project hf-flax
    ```

Cela devrait √©tablir un tunnel SSH dans la VM TPU !
    
> üí° Note : Vous n'√™tes PAS suppos√© avoir acc√®s √† la console Google Cloud. Aussi, il se peut que vous ne receviez pas de lien d'invitation pour rejoindre le projet `hf-flax`. Mais vous devriez tout de m√™me pouvoir acc√©der √† la VM TPU en suivant les √©tapes ci-dessus .     

> Note : Les VM TPU sont d√©j√† attach√©es √† des disques de stockage persistants (de 3 TB). Cela sera utile
au cas o√π votre √©quipe souhaiterait entra√Æner localement un jeu de donn√©es volumineux. Le nom du disque de stockage devrait √©galement figurer dans l'e-mail que vous avez re√ßu. Suivez [cette section](https://github.com/huggingface/community-events/tree/main/jax-controlnet-sprint#mount-a-disk) pour plus de d√©tails.

### Installation de JAX

Commen√ßons par cr√©er un environnement virtuel Python :

```bash
python3 -m venv <your-venv-name>
```

Nous pouvons activer l'environnement en lan√ßant :

```bash
source ~/<your-venv-name>/bin/activate
```

Installez ensuite Diffusers et les d√©pendances d'entra√Ænement de la biblioth√®que :

```bash
pip install git+https://github.com/huggingface/diffusers.git
```

Ensuite, clonez ce d√©p√¥t et installez JAX, Flax et les autres d√©pendances :

```bash
git clone https://github.com/huggingface/community-events
cd community-events/jax-controlnet-sprint/training_scripts
pip install -U -r requirements_flax.txt
```

Pour v√©rifier que JAX a √©t√© correctement install√©, vous pouvez ex√©cuter la commande suivante :

```py
import jax
jax.device_count()
```

Cela devrait afficher le nombre de c≈ìurs de la TPU, qui devrait √™tre de 4 sur une VM TPUv4-8. Si Python n'est pas capable de d√©tecter le p√©riph√©rique TPU, veuillez consulter la section des erreurs possibles plus bas pour des solutions.

Si vous souhaitez utiliser le logging Weights and Biases, vous devez √©galement installer `wandb` maintenant :

```bash
pip install wandb
```

> üí° Note : Weights & Biases est gratuit pour les √©tudiants, les √©ducateurs et les chercheurs universitaires. Tous les participants √† notre √©v√©nement sont qualifi√©s pour obtenir un compte d'√©quipe acad√©mique Weights & Biases. Pour cr√©er votre √©quipe, vous pouvez visiter le site https://wandb.ai/create-team et choisir le type d'√©quipe "*Academic*". Pour plus d'informations sur la cr√©ation et la gestion d'une √©quipe Weights & Biases, vous pouvez consulter le site https://docs.wandb.ai/guides/app/features/teams.
### Ex√©cution du script d'entra√Ænement

Maintenant, t√©l√©chargeons deux images de conditionnement que nous utiliserons pour lancer la validation pendant l'entra√Ænement afin de suivre nos progr√®s

```bash
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png
```

Nous vous encourageons √† stocker ou √† partager votre mod√®le avec la communaut√©. Pour utiliser le Hub, veuillez vous connecter √† votre compte Hugging Face, ou ([en cr√©er un](https://huggingface.co/docs/diffusers/main/en/training/hf.co/join) si vous n'en avez pas d√©j√† un) :

```bash
huggingface-cli login
```

Assurez-vous que les variables d'environnement `MODEL_DIR`, `OUTPUT_DIR` et `HUB_MODEL_ID` sont d√©finies. Les variables `OUTPUT_DIR` et `HUB_MODEL_ID` sp√©cifient o√π sauvegarder le mod√®le sur le Hub :

```bash
export MODEL_DIR="runwayml/stable-diffusion-v1-5"
export OUTPUT_DIR="runs/fill-circle-{timestamp}"
export HUB_MODEL_ID="controlnet-fill-circle"
```

Et enfin, d√©marrez l'entra√Ænement (assurez-vous d'√™tre dans le r√©pertoire `jax-controlnet-sprint/training_scripts`) !

```bash
python3 train_controlnet_flax.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --validation_steps=1000 \
 --train_batch_size=2 \
 --revision="non-ema" \
 --from_pt \
 --report_to="wandb" \
 --tracker_project_name=$HUB_MODEL_ID \
 --num_train_epochs=11 \
 --push_to_hub \
 --hub_model_id=$HUB_MODEL_ID
 ```

Notez que l'argument `--from_pt` convertira votre point de contr√¥le pytorch en flax. Cependant, il ne fonctionnera qu'avec les points de contr√¥le au format diffusers. Si votre `MODEL_DIR` ne contient pas de points de contr√¥le au format diffusers, vous ne pouvez pas utiliser l'argument `--from_pt`. Vous pouvez convertir vos points de contr√¥le `ckpt` ou `safetensors` au format diffusers en utilisant [ce script] (https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py). 

Puisque nous avons pass√© l'argument `--push_to_hub`, il va automatiquement cr√©er un repo de mod√®le sous votre compte Hugging Face bas√© sur `$HUB_MODEL_ID`. √Ä la fin de l'entra√Ænement, le point de contr√¥le final sera automatiquement stock√© sur le Hub. Vous pouvez trouver un exemple de mod√®le [ici](https://huggingface.co/YiYiXu/fill-circle-controlnet).

Notre script d'entra√Ænement fournit √©galement un support limit√© pour le streaming de grands jeux de donn√©es √† partir du Hub. Afin d'activer le streaming, il faut √©galement d√©finir `--max_train_samples`.  Voici un exemple de commande (tir√© de [cet article de blog](https://huggingface.co/blog/train-your-controlnet)) :

```bash
export MODEL_DIR="runwayml/stable-diffusion-v1-5"
export OUTPUT_DIR="runs/uncanny-faces-{timestamp}"
export HUB_MODEL_ID="controlnet-uncanny-faces"

python3 train_controlnet_flax.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=multimodalart/facesyntheticsspigacaptioned \
 --streaming \
 --conditioning_image_column=spiga_seg \
 --image_column=image \
 --caption_column=image_caption \
 --resolution=512 \
 --max_train_samples 100000 \
 --learning_rate=1e-5 \
 --train_batch_size=1 \
 --revision="flax" \
 --report_to="wandb" \
 --tracker_project_name=$HUB_MODEL_ID
```

Notez cependant que les performances des TPUs peuvent √™tre limit√©es car le streaming avec `datasets` n'est pas optimis√© pour les images. Pour assurer un d√©bit maximal, nous vous encourageons √† explorer les options suivantes :

* [Webdataset](https://webdataset.github.io/webdataset/)
* [TorchData](https://github.com/pytorch/data)
* [TensorFlow Datasets](https://www.tensorflow.org/datasets/tfless_tfds)


Lorsque vous travaillez avec un jeu de donn√©es plus important, vous pouvez avoir besoin d'ex√©cuter le processus d'entra√Ænement pendant une longue p√©riode et il est utile d'enregistrer des points de contr√¥le r√©guliers au cours du processus. Vous pouvez utiliser l'argument suivant pour activer les points de contr√¥le interm√©diaires :

```bash
 --checkpointing_steps=500
```
Cela permet d'enregistrer le mod√®le entra√Æn√© dans des sous-dossiers du dossier output_dir. Le nom des sous-dossiers correspond au nombre d'√©tapes effectu√©es jusqu'√† pr√©sent ; par exemple : un point de contr√¥le sauvegard√© apr√®s 500 √©tapes d'entra√Ænement serait sauvegard√© dans un sous-dossier nomm√© 500 

Vous pouvez alors commencer votre entra√Ænement √† partir de ce point de contr√¥le sauvegard√© avec 

```bash
 --controlnet_model_name_or_path="./control_out/500" 
```

Nous soutenons l'entra√Ænement avec la strat√©gie de pond√©ration Min-SNR propos√©e dans [Efficient Diffusion Training via Min-SNR Weighting Strategy](https://arxiv.org/abs/2303.09556) qui permet d'obtenir une convergence plus rapide en r√©√©quilibrant la perte. Pour l'utiliser, il faut d√©finir l'argument `--snr_gamma`. La valeur recommand√©e est `5.0`.

Nous supportons √©galement l'accumulation de gradient, technique qui vous permet d'utiliser une taille de batch plus grande que celle que votre machine serait normalement capable de mettre en m√©moire. Vous pouvez utiliser l'argument `gradient_accumulation_steps` pour d√©finir les √©tapes d'accumulation du gradient. L'auteur de ControlNet recommande d'utiliser l'accumulation de gradient pour obtenir une meilleure convergence. Pour en savoir plus voir [ici](https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md#more-consideration-sudden-converge-phenomenon-and-gradient-accumulation).

Vous pouvez **profiler votre code** avec :

```bash
 --profile_steps==5
```

Reportez-vous √† la [documentation JAX sur le profilage](https://jax.readthedocs.io/en/latest/profiling.html). Pour inspecter la trace de profil, vous devez installer et d√©marrer Tensorboard avec le plugin de profil :

```bash
pip install tensorflow tensorboard-plugin-profile
tensorboard --logdir runs/fill-circle-100steps-20230411_165612/
```

Le profil peut alors √™tre inspect√© √† l'adresse http://localhost:6006/#profile.

Parfois vous obtiendrez des conflits de version (messages d'erreur comme `Duplicate plugins for name projector`), ce qui signifie que vous devez d√©sinstaller et r√©installer toutes les versions de Tensorflow/Tensorboard (par exemple avec `pip uninstall tensorflow tf-nightly tensorboard tb-nightly tensorboard-plugin-profile && pip install tf-nightly tbp-nightly tensorboard-plugin-profile`).

Notez que la fonctionnalit√© de d√©bogage du plugin Tensorboard `profile` est toujours en cours de d√©veloppement. Toutes les vues ne sont pas enti√®rement fonctionnelles, et par exemple le `trace_viewer` coupe les √©v√©nements apr√®s 1M (ce qui peut r√©sulter en la perte de toutes vos traces de p√©riph√©riques si par exemple vous profilez l'√©tape de compilation par accident).

### D√©pannage de votre VM TPU

**TRES IMPORTANT** : Un seul processus peut acc√©der aux c≈ìurs de la TPU √† la fois. Cela signifie que si plusieurs membres de l'√©quipe essaient de se connecter aux c≈ìurs de la TPU, vous obtiendrez des erreurs telles que :

```
libtpu.so already in used by another process. Not attempting to load libtpu.so in this process.
```

Nous recommandons √† chaque membre de l'√©quipe de cr√©er son propre environnement virtuel, mais une seule personne devrait ex√©cuter les processus d'entra√Ænement lourds. De plus, veuillez vous relayer lors de l'installation de la TPUv4-8 afin que tout le monde puisse v√©rifier que JAX est correctement install√©.

Si les membres de votre √©quipe n'utilisent pas actuellement la TPU mais que vous obtenez toujours ce message d'erreur. Vous devez tuer le processus qui utilise la TPU avec :

```
kill -9 PID
```

vous devrez remplacer le terme "PID" par le PID du processus qui utilise TPU. Dans la plupart des cas, cette information est incluse dans le message d'erreur. Par exemple, si vous obtenez 

```
The TPU is already in use by a process with pid 1378725. Not attempting to load libtpu.so in this process.
```

vous pouvez faire

```
kill -9 1378725
```

Vous pouvez √©galement utiliser la commande suivante pour trouver les processus utilisant chacune des puces TPU (par exemple, `/dev/accel0` est l'une des puces TPU)

```
sudo lsof -w /dev/accel0
```

Pour tuer tous les processus √† l'aide de `/dev/accel0`, il faut 

```
sudo lsof -t /dev/accel0 | xargs kill -9
```

Si Python n'est pas capable de d√©tecter votre p√©riph√©rique TPU (i.e. quand vous faites `jax.device_count()` et qu'il sort `0`), cela peut √™tre d√ª au fait que vous n'avez pas les droits d'acc√®s aux logs tpu, ou que vous avez un fichier tpu lock qui tra√Æne. Ex√©cutez les commandes suivantes pour r√©soudre le probl√®me

```
sudo rm -f /tmp/libtpu_lockfile
```

```
sudo chmod o+w /tmp/tpu_logs/
```

## Comment faire une soumission


Pour faire une soumission compl√®te, vous devez avoir les √©l√©ments suivants sur le Hub d'Hugging Face :
- Un d√©p√¥t de mod√®le avec les poids du mod√®le et la carte du mod√®le,
- (Facultatif) Un d√©p√¥t de jeu de donn√©es avec une carte de jeu de donn√©es, 
- Un *Space* qui permet aux autres d'interagir avec votre mod√®le.

### Pousser les poids du mod√®le et la carte du mod√®le vers le Hub

**Si vous utilisez le script d'entra√Ænement (`train_controlnet_flax.py`) fourni dans ce r√©pertoire**

L'activation de l'argument `push_to_hub` dans les arguments d'entra√Ænement va :
- Cr√©er un d√©p√¥t de mod√®les localement et √† distance sur le Hub,
- Cr√©er une carte de mod√®le et l'√©crire dans le d√©p√¥t de mod√®les local,
- Sauvegarder votre mod√®le dans le r√©f√©rentiel de mod√®les local,
- Pousser le d√©p√¥t local vers le Hub.

Votre carte de mod√®le g√©n√©r√©e automatiquement ressemblera √† ceci : 
![Carte de mod√®le](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/jax_model_card.png).

Vous pouvez modifier la carte de mod√®le pour qu'elle soit plus informative. Les cartes de mod√®le qui sont plus informatives que les autres auront plus de poids lors de l'√©valuation.

**Si vous avez entra√Æn√© un mod√®le personnalis√© et que vous n'avez pas utilis√© le script**

Vous devez vous authentifier avec `huggingface-cli login` comme indiqu√© ci-dessus. Si vous utilisez une des classes de mod√®les disponibles dans `diffusers`, sauvegardez votre mod√®le avec la m√©thode `save_pretrained` de votre mod√®le. 

```py
model.save_pretrained("path_to_your_model_repository")
```

Apr√®s avoir sauvegard√© votre mod√®le dans un dossier, vous pouvez simplement utiliser le script ci-dessous pour pousser votre mod√®le vers le Hub : 

```py
from huggingface_hub import create_repo, upload_folder

create_repo("username/my-awesome-model")
upload_folder(
    folder_path="path_to_your_model_repository",
    repo_id="username/my-awesome-model"
)
```

Ceci poussera votre mod√®le vers Hub. Apr√®s avoir pouss√© cela, vous devez cr√©er la carte de mod√®le vous-m√™me. 
Vous pouvez utiliser l'interface graphique pour l'√©diter. 
![Edit Model Card](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/edit_model_card.png)

Chaque carte de mod√®le se compose de deux sections, les m√©tadonn√©es et le texte libre. Vous pouvez √©diter les m√©tadonn√©es √† partir des sections dans l'interface graphique. Si vous avez sauvegard√© votre mod√®le en utilisant `save_pretrained`, vous n'avez pas besoin de fournir `pipeline_tag` et `library_name`. Sinon, fournissez `pipeline_tag`, `library_name` et le jeu de donn√©es s'il existe sur Hugging Face Hub. En plus de cela, vous devez ajouter `jax-diffusers-event` √† la section `tags`.

```
---
license: apache-2.0
library_name: diffusers
tags:
- jax-diffusers-event
datasets:
- red_caps
pipeline_tag: text-to-image
---
```
![Edit Metadata](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/edit_metadata.png)

### Cr√©er notre *Space*

<h4> R√©diger notre application </h4>

Nous utiliserons [Gradio] (https://gradio.app/) pour cr√©er nos applications. Gradio poss√®de deux API principales : `Interface` et `Blocks`. `Interface` est une API de haut niveau qui vous permet de cr√©er une interface avec quelques lignes de code, et `Blocks` est une API de plus bas niveau qui vous donne plus de flexibilit√© sur les interfaces que vous pouvez construire. Le code doit √™tre inclus dans un fichier appel√© `app.py`.

Essayons de cr√©er une application ControlNet comme exemple. L'API `Interface` fonctionne simplement comme suit : 

```py
import gradio as gr

# La fonction d'inf√©rence prend en compte le prompt, le prompt n√©gatif et l'image
def infer(prompt, negative_prompt, image):
    # impl√©mentez votre fonction d'inf√©rence ici
    return output_image

# vous devez passer les entr√©es et les sorties en fonction de la fonction d'inf√©rence
gr.Interface(fn = infer, inputs = ["text", "text", "image"], outputs = "image").launch()
```
Vous pouvez personnaliser votre interface en passant `title`, `description` et `examples` √† la fonction `Interface`.

```py
title = "ControlNet on Canny Filter"
description = "This is a demo on ControlNet based on canny filter."
# vous devez passer vos exemples en fonction de vos entr√©es
# chaque liste int√©rieure est un exemple, chaque √©l√©ment de la liste correspondant √† un composant des `inputs`.
examples = [["a cat with cake texture", "low quality", "cat_image.png"]]
gr.Interface(fn = infer, inputs = ["text", "text", "image"], outputs = "image",
            title = title, description = description, examples = examples, theme='gradio/soft').launch()
```
Votre interface ressemblera √† ceci :
![ControlNet](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio_controlnet.png)

Avec les blocs, vous pouvez ajouter des marques, des onglets, des composants sous les colonnes et les lignes, etc. Supposons que nous ayons deux ControlNets et que nous voulions les inclure dans un *Space*. Nous les placerons sous diff√©rents onglets dans une d√©mo comme ci-dessous : 

```py
import gradio as gr

def infer_segmentation(prompt, negative_prompt, image):
    # votre fonction d'inf√©rence pour le contr√¥le de la segmentation 
    return im

def infer_canny(prompt, negative_prompt, image):
    # votre fonction d'inf√©rence pour un contr√¥le efficace 
    return im

with gr.Blocks(theme='gradio/soft') as demo:
    gr.Markdown("## Stable Diffusion with Different Controls")
    gr.Markdown("In this app, you can find different ControlNets with different filters. ")


    with gr.Tab("ControlNet on Canny Filter "):
        prompt_input_canny = gr.Textbox(label="Prompt")
        negative_prompt_canny = gr.Textbox(label="Negative Prompt")
        canny_input = gr.Image(label="Input Image")
        canny_output = gr.Image(label="Output Image")
        submit_btn = gr.Button(value = "Submit")
        canny_inputs = [prompt_input_canny, negative_prompt_canny, canny_input]
        submit_btn.click(fn=infer_canny, inputs=canny_inputs, outputs=[canny_output])
        
    with gr.Tab("ControlNet with Semantic Segmentation"):
        prompt_input_seg = gr.Textbox(label="Prompt")
        negative_prompt_seg = gr.Textbox(label="Negative Prompt")
        seg_input = gr.Image(label="Image")
        seg_output = gr.Image(label="Output Image")
        submit_btn = gr.Button(value = "Submit")
        seg_inputs = [prompt_input_seg, negative_prompt_seg, seg_input]
        submit_btn.click(fn=infer_segmentation, inputs=seg_inputs, outputs=[seg_output])

demo.launch()
```

La d√©mo ci-dessus ressemblera √† ce qui suit :
![Gradio Blocks](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio_controlnet_blocks.png)


#### Cr√©er notre *Space*
Une fois notre application √©crite, nous pouvons cr√©er un espace Hugging Face pour h√©berger notre application. Vous pouvez aller sur [huggingface.co](http://huggingface.co), cliquer sur votre profil en haut √† droite et s√©lectionner "*New Space*".

![New Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_space.png)

Nous pouvons nommer notre *Space*, choisir une licence et s√©lectionner "Gradio" comme Space SDK. 

![Space Configuration](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/space_config.png)

Apr√®s avoir cr√©√© le *Space*, vous pouvez soit utiliser les instructions ci-dessous pour cloner le d√©p√¥t localement, ajouter vos fichiers et pousser, soit utiliser l'interface graphique pour cr√©er les fichiers et √©crire le code dans le navigateur.

![Spaces Landing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/repository_landing.png)

Pour t√©l√©charger votre fichier de candidature, cliquez sur "*Add File*" et faites glisser votre fichier.

![New Space Landing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/add_file.png)

Enfin, nous devons cr√©er un fichier appel√© `requirements.txt` et ajouter les conditions requises pour notre projet. Assurez-vous d'installer les versions de jax, diffusers et autres d√©pendances comme ci-dessous. 

```py
-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
jax[cuda11_cudnn805]
jaxlib
git+https://github.com/huggingface/diffusers@main
opencv-python
transformers
flax
```

Nous vous accorderons une dotation GPU afin que votre application puisse fonctionner sur GPU.

Nous avons un classement h√©berg√© [ici] (https://huggingface.co/spaces/jax-diffusers-event/leaderboard) et nous distribuerons des prix √† partir de ce classement. Pour que votre *Space* apparaisse sur le leaderboard, √©ditez simplement `README.md` de votre *Space* pour avoir le tag `jax-diffusers-event` sous les tags comme ci-dessous : 
```py
---
title: Canny Coyo1m
emoji: üíú 
...py
tags:
- jax-diffusers-event
---
```

## Prix

Pour ce sprint, nous aurons de nombreux prix. Nous choisirons les dix premiers projets de [ce classement](https://huggingface.co/spaces/jax-diffusers-event/leaderboard), vous devez donc tagger votre *Space* pour le classement afin que votre soumission soit compl√®te, comme indiqu√© dans la section ci-dessus. Les projets sont class√©s en fonction du nombre de j'aimes, nous augmenterons donc partagerons vos Spaces pour en augmenter la visibilit√© pour que les gens puissent voter en laissant un j'aime sur votre Space. Nous s√©lectionnerons les dix premiers projets du classement et le jury votera pour d√©terminer les trois premi√®res places. Ces projets seront mis en valeur par Google et Hugging Face. Les interfaces √©labor√©es ainsi que les projets dont les bases de code et les mod√®les sont en libre acc√®s augmenteront probablement les chances de gagner des prix. 

Les prix sont les suivants et sont remis √† chaque membre de l'√©quipe : 
    
**Premi√®re place** : Un bon d'achat de 150 $ √† d√©penser sur le [*Hugging Face Store*](https://store.huggingface.co/), un abonnement d'un an √† Hugging Face Hub PRO, le livre *Natural Language Processing with Transformers*.
    
**Deuxi√®me place** : Un bon d'achat de 125$ √† d√©penser sur le [*Hugging Face Store*](https://store.huggingface.co/), un abonnement d'un an √† Hugging Face Hub PRO.
    
**Troisi√®me place** : Un bon d'achat de 100 $ √† d√©penser sur le [*Hugging Face Store*](https://store.huggingface.co/), un abonnement d'un an √† Hugging Face Hub PRO.
    
Les dix premiers projets du classement (ind√©pendamment de la d√©cision du jury) gagneront un kit de merch exclusivement con√ßu pour ce sprint par Hugging Face, ainsi qu'un kit de merch s√©par√© JAX de Google. 
    
## Jury
    
Le jury de ce sprint √©tait compos√© des personnes suivantes :
    
1. Robin Rombach, Stability AI
2. Huiwen Chang, Google Research
3. Jun-Yan Zhu, Carnegie Mellon University
4. Merve Noyan, Hugging Face


## FAQ 

Dans cette section, nous rassemblons les r√©ponses aux questions fr√©quemment pos√©es sur notre canal discord.

### Comment utiliser VSCode avec TPU VM ?

Vous pouvez suivre ce [guide g√©n√©ral](https://medium.com/@ivanzhd/vscode-sftp-connection-to-compute-engine-on-google-cloud-platform-gcloud-9312797d56eb) sur la fa√ßon d'utiliser VSCode remote pour se connecter √† Google Cloud VMs. Une fois que c'est configur√©, vous pouvez d√©velopper sur la VM TPU en utilisant VSCode.

Pour obtenir votre IP externe, utilisez cette commande :
```py
gcloud compute tpus tpu-vm describe <node_name> --zone=<zone>
```

Elle devrait √™tre list√©e sous 'accessConfig' -> 'externalIp'

### Comment tester votre code localement ?

Puisque les membres de l'√©quipe partagent la VM TPU, il peut √™tre pratique d'√©crire et de tester votre code localement sur une unit√© centrale pendant que vos co√©quipiers ex√©cutent le processus d'entra√Ænement sur la VM. Pour effectuer des tests locaux, il est important de mettre le drapeau `xla_force_host_platform_device_count` √† `4`. Pour en savoir plus, consultez la [documentation] (https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#aside-hosts-and-devices-in-jax).
    
## Gagnants du sprint
    
Les 10 meilleurs projets (bas√©s sur le nombre de likes sur leurs d√©mos) sont disponibles sur ce [classement](https://huggingface.co/spaces/jax-diffusers-event/leaderboard). Nous avons soumis ce classement √† notre jury pour qu'il juge les 10 meilleurs projets sur la base de plusieurs facteurs tels que les points de contr√¥le du mod√®le, les jeux de donn√©es et les bases de code open-source, l'exhaustivit√© du mod√®le et des cartes de jeux de donn√©es, etc. En cons√©quence, les trois projets suivants sont sortis vainqueurs :
    
1. [ControlNet pour la d√©coration int√©rieure](https://huggingface.co/spaces/controlnet-interior-design/controlnet-seg)
2. [ControlNet pour le r√©glage de la luminosit√©](https://huggingface.co/spaces/ioclab/brightness-controlnet)
3. [Stable Diffusion avec contr√¥le manuel](https://huggingface.co/spaces/vllab/controlnet-hands)