# ãƒ¦ãƒ‹ãƒƒãƒˆ2ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°

Hugging Face Diffusion ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ã‚¹ã®ãƒ¦ãƒ‹ãƒƒãƒˆ2ã¸ã‚ˆã†ã“ã! ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã§ã¯ã€äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ diffusion ãƒ¢ãƒ‡ãƒ«ã‚’æ–°ã—ã„æ–¹æ³•ã§ä½¿ç”¨ã—ã€é©å¿œã•ã›ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚ã¾ãŸã€ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ã€**ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°** ã¨ã—ã¦è¿½åŠ ã®å…¥åŠ›ã‚’å—ã‘ã‚‹ diffusion ãƒ¢ãƒ‡ãƒ«ã‚’ã©ã®ã‚ˆã†ã«ä½œæˆã™ã‚‹ã‹ã‚‚ã”è¦§ã„ãŸã ãã¾ã™ã€‚

## ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã‚’é–‹å§‹ã™ã‚‹ :rocket:

ãƒ¦ãƒ‹ãƒƒãƒˆã®æ‰‹é †ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

- Make sure you've [signed up for this course](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=ef963b4162) so that you can be notified when new material is released
- Read through the material below for an overview of the key ideas of this unit
- Check out the _**Fine-tuning and Guidance**_ notebook to fine-tune an existing diffusion model on a new dataset using the ğŸ¤— Diffusers library and to modify the sampling procedure using guidance
- Follow the example in the notebook to share a Gradio demo for your custom model
- (Optional) Check out the _**Class-conditioned Diffusion Model Example**_ notebook to see how we can add additional control to the generation process.
- (Optional) Check out [this video](https://www.youtube.com/watch?v=mY20iKOQ2zw) for an informal run-through of the material in this unit.


:loudspeaker: Don't forget to join the [Discord](https://huggingface.co/join/discord), where you can discuss the material and share what you've made in the `#diffusion-models-class` channel.

## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

ãƒ¦ãƒ‹ãƒƒãƒˆ1ã§ã”è¦§ã«ãªã£ãŸã‚ˆã†ã«ã€ diffusion ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã‚‚ã®ã§ã™ã€‚ç‰¹ã«é«˜è§£åƒåº¦ã«ãªã‚Œã°ãªã‚‹ã»ã©ã€ã‚¼ãƒ­ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæ™‚é–“ã¨ãƒ‡ãƒ¼ã‚¿ã¯ç¾å®Ÿçš„ã§ã¯ãªããªã‚Šã¾ã™ã€‚å¹¸ã„ã«ã‚‚ã€è§£æ±ºç­–ãŒã‚ã‚Šã¾ã™ï¼šã™ã§ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã™ã€‚ã“ã®æ–¹æ³•ã§ã¯ã€ã‚ã‚‹ç¨®ã®ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»ã‚’ã™ã§ã«å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹ã‚ˆã‚Šã‚‚è‰¯ã„å‡ºç™ºç‚¹ã«ãªã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚

![Example images generated with a model trained on LSUN Bedrooms and fine-tuned for 500 steps on WikiArt](https://api.wandb.ai/files/johnowhitaker/dm_finetune/2upaa341/media/images/Sample%20generations_501_d980e7fe082aec0dfc49.png)

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é€šå¸¸ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å…ƒã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹ç¨‹åº¦ä¼¼ã¦ã„ã‚‹å ´åˆã«ã†ã¾ãã„ãã¾ã™ãŒï¼ˆä¾‹ãˆã°ã€ã‚¢ãƒ‹ãƒ¡ã®é¡”ã‚’ç”Ÿæˆã—ã‚ˆã†ã¨ã™ã‚‹å ´åˆã€é¡”ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã‚‹ã¨ã‚ˆã„ã§ã—ã‚‡ã†ï¼‰ã€é©šãã“ã¨ã«ã€ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå¤§å¹…ã«å¤‰æ›´ã•ã‚ŒãŸå ´åˆã§ã‚‚ã€ãã®åŠ¹æœã¯æŒç¶šã™ã‚‹ã®ã§ã™ã€‚ä¸Šã®ç”»åƒã¯ã€ [LSUN Bedrooms ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«](https://huggingface.co/google/ddpm-bedroom-256)ã¨ [WikiArt ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://huggingface.co/datasets/huggan/wikiart)ã§500ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸã‚‚ã®ã§ã™ã€‚[å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/diffusion-models-class/blob/main/unit2/finetune_model.py)ã¯ã€ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨ä¸€ç·’ã«å‚è€ƒã¨ã—ã¦æ·»ä»˜ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

ç„¡æ¡ä»¶ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿæˆã•ã‚Œã‚‹ã‚‚ã®ã‚’ã‚ã¾ã‚Šã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã§ããªã„ã€‚æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ï¼ˆè©³ã—ãã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èª¬æ˜ã—ã¾ã™ï¼‰ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã€è¿½åŠ ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ãŒã€ã™ã§ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸç„¡æ¡ä»¶ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¨ã¯ã€ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ã‚’ä½•ã‚‰ã‹ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é–¢æ•°ã«ç…§ã‚‰ã—ã¦è©•ä¾¡ã—ã€æœ€çµ‚çš„ã«ç”Ÿæˆã•ã‚Œã‚‹ç”»åƒãŒã‚ˆã‚Šç§ãŸã¡ã®å¥½ã¿ã«åˆã†ã‚ˆã†ã«ä¿®æ­£ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚

![guidance example image](guidance_eg.png)

ã“ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹é–¢æ•°ã¯ã€ã»ã¨ã‚“ã©ã©ã‚“ãªã‚‚ã®ã§ã‚‚å¯èƒ½ã§ã‚ã‚Šã€å¼·åŠ›ãªæŠ€æ³•ã¨ãªã‚Šã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆã§ã¯ã€å˜ç´”ãªä¾‹ï¼ˆä¸Šã®å‡ºåŠ›ä¾‹ã®ã‚ˆã†ã«è‰²ã‚’åˆ¶å¾¡ã™ã‚‹ï¼‰ã‹ã‚‰ã€ CLIP ã¨å‘¼ã°ã‚Œã‚‹äº‹å‰ã«å­¦ç¿’ã•ã›ãŸå¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®è¨˜è¿°ã«åŸºã¥ã„ã¦ç”Ÿæˆã‚’èª˜å°ã™ã‚‹ä¾‹ã¾ã§æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

## ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°

ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯ç„¡æ¡ä»¶ diffusion ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã„ãã¤ã‹ã®ãƒã‚¤ãƒ«ã‚’å¾—ã‚‹ãŸã‚ã®ç´ æ™´ã‚‰ã—ã„æ–¹æ³•ã§ã™ãŒã€ã‚‚ã—å­¦ç¿’ä¸­ã«åˆ©ç”¨å¯èƒ½ãªè¿½åŠ æƒ…å ±ï¼ˆã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚„ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãªã©ï¼‰ãŒã‚ã‚Œã°ã€ãã‚Œã‚’ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã€äºˆæ¸¬ã‚’è¡Œã†éš›ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚ãã†ã™ã‚‹ã“ã¨ã§ã€ **æ¡ä»¶ä»˜ã** ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€æ¨è«–æ™‚ã«æ¡ä»¶ä»˜ã‘ã¨ã—ã¦å…¥åŠ›ã•ã‚Œã‚‹ã‚‚ã®ã‚’åˆ¶å¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã¯ã€ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã«å¾“ã£ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹ã‚¯ãƒ©ã‚¹æ¡ä»¶ä»˜ããƒ¢ãƒ‡ãƒ«ã®ä¾‹ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

![conditioning example](conditional_digit_generation.png)

There are a number of ways to pass in this conditioning information, such as
- Feeding it in as additional channels in the input to the UNet. This is often used when the conditioning information is the same shape as the image, such as a segmentation mask, a depth map or a blurry version of the image (in the case of a restoration/superresolution model). It does work for other types of conditioning too. For example, in the notebook, the class label is mapped to an embedding and then expanded to be the same width and height as the input image so that it can be fed in as additional channels.
- Creating an embedding and then projecting it down to a size that matches the number of channels at the output of one or more internal layers of the UNet, and then adding it to those outputs. This is how the timestep conditioning is handled, for example. The output of each Resnet block has a projected timestep embedding added to it. This is useful when you have a vector such as a CLIP image embedding as your conditioning information. A notable example is the ['Image Variations' version of Stable Diffusion](https://huggingface.co/spaces/lambdalabs/stable-diffusion-image-variations) which does exactly this.
- Adding cross-attention layers that can 'attend' to a sequence passed in as conditioning. This is most useful when the conditioning is in the form of some text - the text is mapped to a sequence of embeddings using a transformer model, and then cross-attention layers in the UNet are used to incorporate this information into the denoising path. We'll see this in action in Unit 3 as we examine how Stable Diffusion handles text conditioning.


## ãƒãƒ³ã‚ºã‚ªãƒ³ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

| Chapter                                     | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |
|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tuning and Guidance                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)              |
| Class-conditioned Diffusion Model Example                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb)              |

At this point, you know enough to get started with the accompanying notebooks! Open them in your platform of choice using the links above. Fine-tuning is quite computationally intensive, so if you're using Kaggle or Google Colab make sure you set the runtime type to 'GPU' for the best results.

The bulk of the material is in _**Fine-tuning and Guidance**_, where we explore these two topics through worked examples. The notebook shows how you can fine-tune an existing model on new data, add guidance, and share the result as a Gradio demo. There is an accompanying script ([finetune_model.py](https://github.com/huggingface/diffusion-models-class/blob/main/unit2/finetune_model.py)) that makes it easy to experiment with different fine-tuning settings, and [an [example space](https://huggingface.co/spaces/johnowhitaker/color-guided-wikiart-diffusion) that you can use as a template for sharing your own demo on ğŸ¤— Spaces.

In the _**Class-conditioned Diffusion Model Example**_, we show a brief worked example of creating a diffusion model conditioned on class labels using the MNIST dataset. The focus is on demonstrating the core idea as simply as possible: by giving the model extra information about what it is supposed to be denoising, we can later control what kinds of images are generated at inference time.

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ 

Following the examples in the _**Fine-tuning and Guidance**_ notebook, fine-tune your own model or pick an existing model and create a Gradio demo to showcase your new guidance skills. Don't forget to share your demo on Discord, Twitter etc so we can admire your work!

## è¿½åŠ è³‡æ–™

[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) - Introduced the DDIM sampling method (used by DDIMScheduler)

[GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) - Introduced methods for conditioning diffusion models on text

[eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers](https://arxiv.org/abs/2211.01324) - Shows how many different kinds of conditioning can be used together to give even more control over the kinds of samples generated

ã‚‚ã£ã¨ç´ æ™´ã‚‰ã—ã„ãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã“ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚
