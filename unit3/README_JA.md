# ãƒ¦ãƒ‹ãƒƒãƒˆ3: Stable Diffusion

Hugging Face Diffusion ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ¼ã‚¹ã®ãƒ¦ãƒ‹ãƒƒãƒˆ3ã¸ã‚ˆã†ã“ãï¼ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã§ã¯ã€Stable Diffusion (SD) ã¨å‘¼ã°ã‚Œã‚‹å¼·åŠ›ãª diffusion ãƒ¢ãƒ‡ãƒ«ã«å‡ºä¼šã„ã€ãã‚ŒãŒã§ãã‚‹ã“ã¨ã‚’æ¢ã‚Šã¾ã™ã€‚

## ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã‚’é–‹å§‹ã™ã‚‹ :rocket:

ãƒ¦ãƒ‹ãƒƒãƒˆã®æ‰‹é †ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:

- æ–°ã—ã„æ•™æãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã¨ãã«é€šçŸ¥ã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã€[ã“ã®ã‚³ãƒ¼ã‚¹ã«ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=ef963b4162)ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
- ä»¥ä¸‹ã®è³‡æ–™ã«ç›®ã‚’é€šã—ã€æœ¬ãƒ¦ãƒ‹ãƒƒãƒˆã®é‡è¦ãªè€ƒãˆæ–¹ã®æ¦‚è¦ã‚’ç†è§£ã—ã¦ãã ã•ã„
- [_**Stable Diffusion Introduction**_ notebook](#hands-on-notebook) ã§ã¯ã€SD ã‚’å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’ã”ç´¹ä»‹ã—ã¦ã„ã¾ã™
- Use the _**Dreambooth**_ notebook in the [**hackathon** folder](https://github.com/huggingface/diffusion-models-class/tree/main/hackathon) to fine-tune your own custom Stable Diffusion model and share it with the community for a chance to win some prizes and swag
- (Optional) Check out the [_**Stable Diffusion Deep Dive video**_](https://www.youtube.com/watch?app=desktop&v=0_BBRNYInx8) and the accompanying [_**notebook**_](https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb) for a deeper exploration of the different components and how they can be adapted for different effects. This material was created for the new FastAI course, ['Stable Diffusion from the Foundations'](https://www.fast.ai/posts/part2-2022.html) - the first few lessons are already available and the rest will be released in the next few months, making this a great supplement to this class for anyone curious about building these kinds of models completely from scratch.


:loudspeaker: [Discord](https://huggingface.co/join/discord) ã«å‚åŠ ã™ã‚‹ã®ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚ã“ã“ã§ã¯ã€ `#diffusion-models-class` ãƒãƒ£ãƒ³ãƒãƒ«ã§æ•™æã«ã¤ã„ã¦è­°è«–ã—ãŸã‚Šã€ä½œã£ãŸã‚‚ã®ã‚’å…±æœ‰ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## ã¯ã˜ã‚ã«

![SD example images](sd_demo_images.jpg)<br>
_Stable Diffusion ã§ç”Ÿæˆã—ãŸç”»åƒä¾‹_

Stable Diffusion ã¯ã€å¼·åŠ›ãªãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãæ½œåœ¨çš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¿ƒé…ã—ãªã„ã§ãã ã•ã„ã€ã“ã®è¨€è‘‰ã«ã¤ã„ã¦ã¯ã™ãã«èª¬æ˜ã—ã¾ã™ ãƒ†ã‚­ã‚¹ãƒˆã®è¨˜è¿°ã‹ã‚‰ç´ æ™´ã‚‰ã—ã„ç”»åƒã‚’ä½œæˆã™ã‚‹ãã®èƒ½åŠ›ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã§ã‚»ãƒ³ã‚»ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å·»ãèµ·ã“ã—ã¾ã—ãŸã€‚ã“ã®ãƒ¦ãƒ‹ãƒƒãƒˆã§ã¯ã€ SD ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’æ¢ã‚Šã€ä»–ã«ã©ã®ã‚ˆã†ãªãƒˆãƒªãƒƒã‚¯ãŒã§ãã‚‹ã‹ã‚’è¦‹ã¦ã„ãã“ã¨ã«ã—ã¾ã™ã€‚

## Latent Diffusion

ç”»åƒã®ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã¨ã€ãã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè¨ˆç®—èƒ½åŠ›ã‚‚å¤§ãããªã‚Šã¾ã™ã€‚ç‰¹ã«è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨å‘¼ã°ã‚Œã‚‹æ“ä½œã§ã¯é¡•è‘—ã§ã€å…¥åŠ›æ•°ã«å¯¾ã—ã¦äºŒæ¬¡é–¢æ•°çš„ã«æ“ä½œé‡ãŒå¢—ãˆã¦ã„ãã¾ã™ã€‚128 px ã®æ­£æ–¹å½¢ç”»åƒã¯64 px ã®æ­£æ–¹å½¢ç”»åƒã®4å€ã®ç”»ç´ æ•°ã‚’æŒã¤ãŸã‚ã€è‡ªå·±ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã§ã¯16å€ï¼ˆã¤ã¾ã‚Š4<sup>2</sup>ï¼‰ã®ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã¯ã€é«˜è§£åƒåº¦ã®ç”»åƒã‚’ç”Ÿæˆã—ãŸã„äººã«ã¨ã£ã¦ã®å•é¡Œã«ãªã‚Šã¾ã™ï¼

![latent diffusion diagram](https://github.com/CompVis/latent-diffusion/raw/main/assets/modelfigure.png)<br>
_Diagram from the [Latent Diffusion paper](http://arxiv.org/abs/2112.10752)_

ã“ã®å•é¡Œã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€ VAEï¼ˆVariational Auto-Encoderï¼‰ ã¨å‘¼ã°ã‚Œã‚‹åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€ç”»åƒã‚’ã‚ˆã‚Šå°ã•ãªç©ºé–“æ¬¡å…ƒã«åœ§ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ååˆ†ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ã€ VAE ã¯å…¥åŠ›ç”»åƒã‚’ã‚ˆã‚Šå°ã•ãè¡¨ç¾ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã€ã“ã®å°ã•ãª **æ½œåœ¨çš„** è¡¨ç¾ã«åŸºã¥ã„ã¦ç”»åƒã‚’å¿ å®Ÿã«å†æ§‹æˆã§ãã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ SD ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ VAE ã¯ã€3ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒã‚’å–ã‚Šè¾¼ã¿ã€å„ç©ºé–“æ¬¡å…ƒã®ç¸®å°ç‡ã‚’8ã¨ã—ãŸ4ãƒãƒ£ãƒ³ãƒãƒ«ã®æ½œåƒè¡¨ç¾ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€512 px ã®æ­£æ–¹å½¢ã®å…¥åŠ›ç”»åƒã¯ã€4 x 64 x 64ã®æ½œåƒã«åœ§ç¸®ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚

By applying the diffusion process on these **latent representations** rather than on full-resolution images, we can get many of the benefits that would come from using smaller images (lower memory usage, fewer layers needed in the UNet, faster generation times...) and still decode the result back to a high-resolution image once we're ready to view the final result. This innovation dramatically lowers the cost to train and run these models.

## ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°

In Unit 2 we showed how feeding additional information to the UNet allows us to have some additional control over the types of images generated. We call this conditioning. Given a noisy version of an image, the model is tasked with predicting the denoised version **based on additional clues** such as a class label or, in the case of Stable Diffusion, a text description of the image. At inference time, we can feed in the description of an image we'd like to see and some pure noise as a starting point, and the model does its best to 'denoise' the random input into something that matches the caption. 

![text encoder diagram](text_encoder_noborder.png)<br>
_Diagram showing the text encoding process which transforms the input prompt into a set of text embeddings (the encoder_hidden_states) which can then be fed in as conditioning to the UNet._

For this to work, we need to create a numeric representation of the text that captures relevant information about what it describes. To do this, SD leverages a pre-trained transformer model based on something called CLIP. CLIP's text encoder was designed to process image captions into a form that could be used to compare images and text, so it is well suited to the task of creating useful representations from image descriptions. An input prompt is first tokenized (based on a large vocabulary where each word or sub-word is assigned a specific token) and then fed through the CLIP text encoder, producing a 768-dimensional (in the case of SD 1.X) or 1024-dimensional (SD 2.X) vector for each token. To keep things consistent prompts are always padded/truncated to be 77 tokens long, and so the final representation which we use as conditioning is a tensor of shape 77x1024 per prompt.

![conditioning diagram](sd_unet_color.png)

OK, so how do we actually feed this conditioning information into the UNet for it to use as it makes predictions? The answer is something called cross-attention. Scattered throughout the UNet are cross-attention layers. Each spatial location in the UNet can 'attend' to different tokens in the text conditioning, bringing in relevant information from the prompt. The diagram above shows how this text conditioning (as well as timestep-based conditioning) is fed in at different points. As you can see, at every level the UNet has ample opportunity to make use of this conditioning!

## Classifier-free ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

It turns out that even with all of the effort put into making the text conditioning as useful as possible, the model still tends to default to relying mostly on the noisy input image rather than the prompt when making its predictions. In a way, this makes sense - many captions are only loosely related to their associated images and so the model learns not to rely too heavily on the descriptions! However, this is undesirable when it comes time to generate new images - if the model doesn't follow the prompt then we may get images out that don't relate to our description at all.

![CFG scale demo grid](cfg_example_0_1_2_10.jpeg)<br>
_Images generated from the prompt "An oil painting of a collie in a top hat" with CFG scale 0, 1, 2 and 10 (left to right)_

To fix this, we use a trick called Classifier-Free Guidance (CGF). During training, text conditioning is sometimes kept blank, forcing the model to learn to denoise images with no text information whatsoever (unconditional generation). Then at inference time, we make two separate predictions: one with the text prompt as conditioning and one without. We can then use the difference between these two predictions to create a final combined prediction that pushes **even further** in the direction indicated by the text-conditioned prediction according to some scaling factor (the guidance scale), hopefully resulting in an image that better matches the prompt. The image above shows the outputs for a prompt at different guidance scales - as you can see, higher values result in images that better match the description.

## ãã®ä»–ã®ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã®ç¨®é¡: Super-Resolution, InpaintingåŠã³Depth-to-Image

It is possible to create versions of Stable Diffusion that take in additional kinds of conditioning. For example, the [Depth-to-Image model](https://huggingface.co/stabilityai/stable-diffusion-2-depth) has extra input channels that take in-depth information about the image being denoised, and at inference time we can feed in the depth map of a target image (estimated using a separate model) to hopefully generate an image with a similar overall structure.

![depth to image example](https://huggingface.co/stabilityai/stable-diffusion-2-depth/resolve/main/depth2image.png)<br>
_Depth-conditioned SD is able to generate different images with the same overall structure (example from StabilityAI)_

In a similar manner, we can feed in a low-resolution image as the conditioning and have the model generate the high-resolution version ([as used by the Stable Diffusion Upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler)). Finally, we can feed in a mask showing a region of the image to be re-generated as part of the 'in-painting' task, where the non-mask regions need to stay intact while new content is generated for the masked area.

## DreamBooth ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

![dreambooth diagram](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)
_Image from the [dreambooth project page](https://dreambooth.github.io/) based on the Imagen model_

DreamBooth is a technique for fine-tuning a text-to-image model to 'teach' it a new concept, such as a specific object or style. The technique was originally developed for Google's Imagen model but was quickly adapted to [work for stable diffusion](https://huggingface.co/docs/diffusers/training/dreambooth). Results can be extremely impressive (if you've seen anyone with an AI profile picture on social media recently the odds are high it came from a dreambooth-based service) but the technique is also sensitive to the settings used, so check out our notebook and [this great investigation into the different training parameters](https://huggingface.co/blog/dreambooth) for some tips on getting it working as well as possible.

## ãƒãƒ³ã‚ºã‚ªãƒ³ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

| ç«                                      | Colab                                                                                                                                                                                               | Kaggle                                                                                                                                                                                                   | Gradient                                                                                                                                                                               | Studio Lab                                                                                                                                                                                                   |
|:--------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Stable Diffusion Introduction                                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit3/01_stable_diffusion_introduction.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/unit3/01_stable_diffusion_introduction.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/unit3/01_stable_diffusion_introduction.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/unit3/01_stable_diffusion_introduction.ipynb)              |
| DreamBooth Hackathon Notebook                      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/hackathon/dreambooth.ipynb)              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/huggingface/diffusion-models-class/blob/main/hackathon/dreambooth.ipynb)              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/huggingface/diffusion-models-class/blob/main/hackathon/dreambooth.ipynb)              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/diffusion-models-class/blob/main/hackathon/dreambooth.ipynb)              |
| Stable Diffusion Deep Dive                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://console.paperspace.com/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              | [![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb )              |

ã“ã®æ™‚ç‚¹ã§ã€ä»˜å±ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ã„å§‹ã‚ã‚‹ã®ã«ååˆ†ãªçŸ¥è­˜ãŒã‚ã‚Šã¾ã™ã€‚ä¸Šã®ãƒªãƒ³ã‚¯ã‹ã‚‰å¥½ããªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§é–‹ã„ã¦ãã ã•ã„ã€‚ Dreambooth ã¯ã‹ãªã‚Šã®è¨ˆç®—èƒ½åŠ›ã‚’å¿…è¦ã¨ã™ã‚‹ã®ã§ã€ Kaggle ã‚„ Google Colab ã‚’ä½¿ã†å ´åˆã¯ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¿ã‚¤ãƒ—ã‚’ 'GPU' ã«è¨­å®šã—ã¦ãŠãã¨ã‚ˆã„ã§ã—ã‚‡ã†ã€‚

'Stable Diffusion Introduction' ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ ğŸ¤— Diffusers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸ stable diffusion ã®ç°¡å˜ãªç´¹ä»‹ã§ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ã¦ç”»åƒã‚’ç”Ÿæˆãƒ»ä¿®æ­£ã™ã‚‹åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

DreamBooth Hackathon Notebook ï¼ˆ[ãƒãƒƒã‚«ã‚½ãƒ³ãƒ•ã‚©ãƒ«ãƒ€](https://github.com/huggingface/diffusion-models-class/tree/main/hackathon)å†…ï¼‰ã«ã¯ã€æ–°ã—ã„ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€è‡ªåˆ†ã®ç”»åƒã§ SD ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

æœ€å¾Œã«ã€ 'Stable Diffusion Deep Dive' ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨ãƒ“ãƒ‡ã‚ªã§ã¯ã€å…¸å‹çš„ãªç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’åˆ†è§£ã—ã€å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å¤‰æ›´ã—ã¦ã•ã‚‰ã«å‰µé€ çš„ãªåˆ¶å¾¡ã‚’è¡Œã†ãŸã‚ã®æ–¬æ–°ãªæ–¹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚


## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ 

**DreamBooth** ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®1ã¤ã«ã¤ã„ã¦ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚å„ã‚«ãƒ†ã‚´ãƒªã§æœ€ã‚‚å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ãŸã‚ã«ã€å¿…ãšå‡ºåŠ›ä¾‹ã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚è³å“ã‚„GPUã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãªã©ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒãƒƒã‚«ã‚½ãƒ³æƒ…å ±](https://github.com/huggingface/diffusion-models-class/tree/main/hackathon)ã‚’ã”è¦§ãã ã•ã„ã€‚

## è¿½åŠ è³‡æ–™

- [High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752) - Stable Diffusion ã®èƒŒå¾Œã«ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç´¹ä»‹ã—ãŸè«–æ–‡

- [CLIP](https://openai.com/blog/clip/) - CLIP ã¯ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®æ¥ç¶šã‚’å­¦ç¿’ã—ã€ CLIP ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’SDã§ä½¿ç”¨ã•ã‚Œã‚‹è±Šå¯Œãªæ•°å€¤è¡¨ç¾ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚æœ€è¿‘ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã® CLIP ã®äºœç¨®ï¼ˆãã®ã†ã¡ã®1ã¤ãŒ SD ãƒãƒ¼ã‚¸ãƒ§ãƒ³2ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼‰ã®èƒŒæ™¯ã«ã¤ã„ã¦ã¯ã€ [OpenCLIP ã«é–¢ã™ã‚‹ã“ã®è¨˜äº‹](https://wandb.ai/johnowhitaker/openclip-benchmarking/reports/Exploring-OpenCLIP--VmlldzoyOTIzNzIz)ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚

- [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ CFG ã‚’å®Ÿè¨¼ã—ãŸåˆæœŸã®è«–æ–‡

ã‚‚ã£ã¨ç´ æ™´ã‚‰ã—ã„ãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã“ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚
